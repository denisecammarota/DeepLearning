{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import models as models\n",
    "import activations as activations\n",
    "import optimizers as optimizers\n",
    "import losses as losses\n",
    "import metrics as metrics\n",
    "import layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basicamente funciona la situacion 1 del ejercicio anterior\n",
    "#que es este ejercicio, asi que me enfoco en generar los datos\n",
    "\n",
    "#funciona ok. verificado\n",
    "#funcion para crear los datos\n",
    "def create_parity_data(n):\n",
    "    x = list(product(range(-1,2,2), repeat=n))\n",
    "    x = np.array(x)\n",
    "    y = np.reshape(np.prod(x,axis=1),(x.shape[0],1)) #calculo paridad y hago reshape para que el formato este correcto\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:  1.4777050552582487 0.0\n",
      "training data:  1.3656881210194323 0.0\n",
      "training data:  1.24523445972629 0.0\n",
      "training data:  1.1377796350088711 0.0\n",
      "training data:  1.064011599479771 0.0\n",
      "training data:  1.025378345752569 0.0\n",
      "training data:  1.0086855953536737 0.0\n",
      "training data:  1.0020241374609737 0.0\n",
      "training data:  0.9992664831044306 0.0\n",
      "training data:  0.9978860618196275 0.0\n",
      "training data:  0.9969138194276056 0.0\n",
      "training data:  0.9959725321631347 0.0\n",
      "training data:  0.994896394011213 0.0\n",
      "training data:  0.9935889534169647 0.0\n",
      "training data:  0.9919699018590116 0.0\n",
      "training data:  0.9899538382849885 0.0\n",
      "training data:  0.9874401788721227 0.0\n",
      "training data:  0.9843066736153762 0.0\n",
      "training data:  0.9804038507605846 0.0\n",
      "training data:  0.9755495555897914 0.0\n",
      "training data:  0.9695235816215939 0.0\n",
      "training data:  0.9620629631328076 0.0\n",
      "training data:  0.9528591060488132 0.0\n",
      "training data:  0.9415586794922242 0.0\n",
      "training data:  0.9277710078213274 0.0\n",
      "training data:  0.9110852904867847 0.0\n",
      "training data:  0.8911007109374556 0.0\n",
      "training data:  0.8674705199465493 0.0\n",
      "training data:  0.8399569347871402 0.0\n",
      "training data:  0.8084879499820333 0.0\n",
      "training data:  0.7732027320834328 0.0\n",
      "training data:  0.7344728210532797 0.0\n",
      "training data:  0.6928927839758129 0.0\n",
      "training data:  0.6492422253643958 0.0\n",
      "training data:  0.6044259114659912 0.0\n",
      "training data:  0.5593998033962583 0.0\n",
      "training data:  0.5150917027777333 0.0\n",
      "training data:  0.47232741070420703 0.0\n",
      "training data:  0.4317740741548355 0.0\n",
      "training data:  0.3939091468134966 0.0\n",
      "training data:  0.359016879259527 0.0\n",
      "training data:  0.32720761823692784 0.0\n",
      "training data:  0.29845124402135814 0.0\n",
      "training data:  0.27261562901302455 0.0\n",
      "training data:  0.24950307718743342 0.0\n",
      "training data:  0.22888068809757223 0.0\n",
      "training data:  0.21050323330513454 0.0\n",
      "training data:  0.19412889291017651 0.0\n",
      "training data:  0.17952907583583036 0.0\n",
      "training data:  0.16649378899460768 0.0\n",
      "training data:  0.15483390596866634 0.0\n",
      "training data:  0.14438142470780507 0.0\n",
      "training data:  0.13498852022227925 0.0\n",
      "training data:  0.1265259509475124 0.0\n",
      "training data:  0.11888118495552435 0.0\n",
      "training data:  0.11195647289894742 0.0\n",
      "training data:  0.10566699905515928 0.0\n",
      "training data:  0.09993917919089741 0.0\n",
      "training data:  0.09470913458541061 0.0\n",
      "training data:  0.08992134793370123 0.0\n",
      "training data:  0.08552749354576966 0.0\n",
      "training data:  0.08148542747751153 0.0\n",
      "training data:  0.07775832043683664 0.0\n",
      "training data:  0.07431391585167293 0.0\n",
      "training data:  0.07112389631525554 0.0\n",
      "training data:  0.0681633430896569 0.0\n",
      "training data:  0.06541027506208741 0.0\n",
      "training data:  0.0628452552867311 0.0\n",
      "training data:  0.0604510548873934 0.0\n",
      "training data:  0.058212365585352185 0.0\n",
      "training data:  0.056115553431808116 0.0\n",
      "training data:  0.05414844746542262 0.0\n",
      "training data:  0.052300157993825966 0.0\n",
      "training data:  0.05056092002995742 0.0\n",
      "training data:  0.04892195811755014 0.0\n",
      "training data:  0.04737536937257507 0.0\n",
      "training data:  0.045914022065269795 0.0\n",
      "training data:  0.04453146748503516 0.0\n",
      "training data:  0.043221863180649335 0.0\n",
      "training data:  0.04197990596182098 0.0\n",
      "training data:  0.04080077329432168 0.0\n",
      "training data:  0.03968007192760364 0.0\n",
      "training data:  0.03861379276745767 0.0\n",
      "training data:  0.03759827115235929 0.0\n",
      "training data:  0.0366301518152406 0.0\n",
      "training data:  0.03570635791629878 0.0\n",
      "training data:  0.034824063620254475 0.0\n",
      "training data:  0.033980669765828 0.0\n",
      "training data:  0.03317378223827734 0.0\n",
      "training data:  0.03240119270945389 0.0\n",
      "training data:  0.03166086145548462 0.0\n",
      "training data:  0.030950902001139355 0.0\n",
      "training data:  0.030269567373236866 0.0\n",
      "training data:  0.029615237773958927 0.0\n",
      "training data:  0.028986409509409918 0.0\n",
      "training data:  0.02838168502979752 0.0\n",
      "training data:  0.027799763955727537 0.0\n",
      "training data:  0.027239434980744257 0.0\n",
      "training data:  0.026699568553764153 0.0\n",
      "training data:  0.02617911025676091 0.0\n",
      "training data:  0.025677074803216692 0.0\n",
      "training data:  0.025192540591686526 0.0\n",
      "training data:  0.02472464475651016 0.0\n",
      "training data:  0.024272578664412275 0.0\n",
      "training data:  0.023835583811590864 0.0\n",
      "training data:  0.023412948081020613 0.0\n",
      "training data:  0.02300400232419253 0.0\n",
      "training data:  0.022608117235456963 0.0\n",
      "training data:  0.0222247004906064 0.0\n",
      "training data:  0.021853194124390223 0.0\n",
      "training data:  0.02149307212434842 0.0\n",
      "training data:  0.021143838220731702 0.0\n",
      "training data:  0.02080502385438164 0.0\n",
      "training data:  0.02047618630630911 0.0\n",
      "training data:  0.020156906974364848 0.0\n",
      "training data:  0.019846789783865233 0.0\n",
      "training data:  0.01954545972034447 0.0\n",
      "training data:  0.019252561473768328 0.0\n",
      "training data:  0.018967758184583697 0.0\n",
      "training data:  0.01869073028290531 0.0\n",
      "training data:  0.018421174412969928 0.0\n",
      "training data:  0.018158802435731515 0.0\n",
      "training data:  0.0179033405031347 0.0\n",
      "training data:  0.017654528198201652 0.0\n",
      "training data:  0.01741211773560384 0.0\n",
      "training data:  0.017175873217871247 0.0\n",
      "training data:  0.016945569942827157 0.0\n",
      "training data:  0.016720993758227005 0.0\n",
      "training data:  0.01650194045993291 0.0\n",
      "training data:  0.016288215230274863 0.0\n",
      "training data:  0.01607963211353754 0.0\n",
      "training data:  0.015876013525772538 0.0\n",
      "training data:  0.015677189796372767 0.0\n",
      "training data:  0.015482998739059294 0.0\n",
      "training data:  0.01529328525012668 0.0\n",
      "training data:  0.01510790093196824 0.0\n",
      "training data:  0.014926703740064873 0.0\n",
      "training data:  0.014749557651766491 0.0\n",
      "training data:  0.01457633235532868 0.0\n",
      "training data:  0.014406902957789184 0.0\n",
      "training data:  0.014241149710378959 0.0\n",
      "training data:  0.014078957750264812 0.0\n",
      "training data:  0.01392021685751266 0.5\n",
      "training data:  0.013764821226245302 0.5\n",
      "training data:  0.013612669249046356 0.5\n",
      "training data:  0.013463663313733339 0.5\n",
      "training data:  0.013317709611687278 0.5\n",
      "training data:  0.013174717956987193 0.5\n",
      "training data:  0.013034601615651793 0.5\n",
      "training data:  0.01289727714434203 0.5\n",
      "training data:  0.012762664237923944 0.5\n",
      "training data:  0.012630685585334683 0.5\n",
      "training data:  0.012501266733233304 0.5\n",
      "training data:  0.012374335956954977 0.5\n",
      "training data:  0.01224982413832015 0.5\n",
      "training data:  0.012127664649881357 0.5\n",
      "training data:  0.012007793245219172 0.5\n",
      "training data:  0.011890147954924548 0.5\n",
      "training data:  0.011774668987930068 0.5\n",
      "training data:  0.01166129863787435 0.5\n",
      "training data:  0.01154998119420543 0.5\n",
      "training data:  0.011440662857747873 0.5\n",
      "training data:  0.011333291660476643 0.5\n",
      "training data:  0.011227817389257275 0.5\n",
      "training data:  0.011124191513327126 0.5\n",
      "training data:  0.011022367115307664 0.5\n",
      "training data:  0.010922298825549685 0.5\n",
      "training data:  0.010823942759627536 0.5\n",
      "training data:  0.010727256458808183 0.5\n",
      "training data:  0.010632198833333103 0.5\n",
      "training data:  0.010538730108360261 0.5\n",
      "training data:  0.01044681177242272 0.5\n",
      "training data:  0.01035640652826977 0.5\n",
      "training data:  0.010267478245963512 0.5\n",
      "training data:  0.01017999191811261 0.5\n",
      "training data:  0.010093913617130964 0.5\n",
      "training data:  0.010009210454416217 0.5\n",
      "training data:  0.009925850541348997 0.5\n",
      "training data:  0.00984380295201948 0.5\n",
      "training data:  0.009763037687593673 0.5\n",
      "training data:  0.009683525642236027 0.5\n",
      "training data:  0.009605238570510662 0.5\n",
      "training data:  0.009528149056187282 0.5\n",
      "training data:  0.009452230482382184 0.5\n",
      "training data:  0.009377457002968714 0.5\n",
      "training data:  0.009303803515195054 0.5\n",
      "training data:  0.00923124563345082 0.5\n",
      "training data:  0.009159759664126887 0.5\n",
      "training data:  0.009089322581516188 0.5\n",
      "training data:  0.00901991200470581 0.5\n",
      "training data:  0.008951506175413496 0.5\n",
      "training data:  0.008884083936724217 0.5\n",
      "training data:  0.008817624712684699 0.5\n",
      "training data:  0.008752108488716062 0.5\n",
      "training data:  0.008687515792806823 0.5\n",
      "training data:  0.008623827677450607 0.5\n",
      "training data:  0.008561025702294467 0.5\n",
      "training data:  0.008499091917465708 0.5\n",
      "training data:  0.008438008847546636 0.5\n",
      "training data:  0.008377759476168395 0.5\n",
      "training data:  0.008318327231196137 0.5\n",
      "training data:  0.008259695970479552 0.5\n",
      "training data:  0.008201849968143791 0.5\n",
      "training data:  0.00814477390139744 0.5\n",
      "training data:  0.008088452837834681 0.5\n",
      "training data:  0.008032872223210738 0.5\n",
      "training data:  0.007978017869669968 0.5\n",
      "training data:  0.0079238759444075 0.5\n",
      "training data:  0.007870432958745942 0.5\n",
      "training data:  0.007817675757609643 0.5\n",
      "training data:  0.007765591509379904 1.0\n",
      "training data:  0.007714167696115167 1.0\n",
      "training data:  0.007663392104121133 1.0\n",
      "training data:  0.007613252814856381 1.0\n",
      "training data:  0.007563738196159584 1.0\n",
      "training data:  0.007514836893785514 1.0\n",
      "training data:  0.0074665378232369635 1.0\n",
      "training data:  0.007418830161880944 1.0\n",
      "training data:  0.0073717033413375835 1.0\n",
      "training data:  0.007325147040130908 1.0\n",
      "training data:  0.007279151176591182 1.0\n",
      "training data:  0.00723370590199874 1.0\n",
      "training data:  0.007188801593959966 1.0\n",
      "training data:  0.007144428850006329 1.0\n",
      "training data:  0.00710057848140774 1.0\n",
      "training data:  0.007057241507192054 1.0\n",
      "training data:  0.007014409148362732 1.0\n",
      "training data:  0.006972072822307143 1.0\n",
      "training data:  0.006930224137388215 1.0\n",
      "training data:  0.006888854887712505 1.0\n",
      "training data:  0.006847957048068039 1.0\n",
      "training data:  0.006807522769025687 1.0\n",
      "training data:  0.006767544372197775 1.0\n",
      "training data:  0.006728014345648287 1.0\n",
      "training data:  0.006688925339448935 1.0\n",
      "training data:  0.00665027016137593 1.0\n",
      "training data:  0.006612041772742031 1.0\n",
      "training data:  0.006574233284359299 1.0\n",
      "training data:  0.006536837952627592 1.0\n",
      "training data:  0.006499849175744275 1.0\n",
      "training data:  0.006463260490031026 1.0\n",
      "training data:  0.0064270655663733215 1.0\n",
      "training data:  0.006391258206768783 1.0\n",
      "training data:  0.006355832340980393 1.0\n",
      "training data:  0.006320782023291081 1.0\n",
      "training data:  0.006286101429355983 1.0\n",
      "training data:  0.0062517848531490425 1.0\n",
      "training data:  0.006217826704000715 1.0\n",
      "training data:  0.0061842215037235335 1.0\n",
      "training data:  0.006150963883822762 1.0\n",
      "training data:  0.00611804858278886 1.0\n",
      "training data:  0.006085470443469377 1.0\n",
      "training data:  0.006053224410517316 1.0\n",
      "training data:  0.006021305527913504 1.0\n",
      "training data:  0.005989708936560467 1.0\n",
      "training data:  0.005958429871945508 1.0\n",
      "training data:  0.005927463661870478 1.0\n",
      "training data:  0.005896805724246348 1.0\n",
      "training data:  0.005866451564950117 1.0\n",
      "training data:  0.005836396775742341 1.0\n",
      "training data:  0.005806637032243013 1.0\n",
      "training data:  0.005777168091964105 1.0\n",
      "training data:  0.00574798579239688 1.0\n",
      "training data:  0.005719086049152195 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:  0.0056904648541520565 1.0\n",
      "training data:  0.005662118273870963 1.0\n",
      "training data:  0.00563404244762524 1.0\n",
      "training data:  0.005606233585908985 1.0\n",
      "training data:  0.005578687968775182 1.0\n",
      "training data:  0.005551401944260381 1.0\n",
      "training data:  0.005524371926851885 1.0\n",
      "training data:  0.0054975943959957915 1.0\n",
      "training data:  0.005471065894644912 1.0\n",
      "training data:  0.0054447830278451705 1.0\n",
      "training data:  0.005418742461359316 1.0\n",
      "training data:  0.005392940920326964 1.0\n",
      "training data:  0.005367375187959544 1.0\n",
      "training data:  0.005342042104269515 1.0\n",
      "training data:  0.005316938564832426 1.0\n",
      "training data:  0.005292061519581132 1.0\n",
      "training data:  0.005267407971630947 1.0\n",
      "training data:  0.005242974976135045 1.0\n",
      "training data:  0.00521875963916911 1.0\n",
      "training data:  0.00519475911664427 1.0\n",
      "training data:  0.005170970613247724 1.0\n",
      "training data:  0.00514739138140998 1.0\n",
      "training data:  0.005124018720298149 1.0\n",
      "training data:  0.005100849974834417 1.0\n",
      "training data:  0.005077882534738986 1.0\n",
      "training data:  0.005055113833596794 1.0\n",
      "training data:  0.00503254134794732 1.0\n",
      "training data:  0.005010162596396886 1.0\n",
      "training data:  0.004987975138752621 1.0\n",
      "training data:  0.004965976575177687 1.0\n",
      "training data:  0.004944164545367111 1.0\n",
      "training data:  0.004922536727743503 1.0\n",
      "training data:  0.004901090838672312 1.0\n",
      "training data:  0.004879824631695895 1.0\n",
      "training data:  0.004858735896786008 1.0\n",
      "training data:  0.004837822459614131 1.0\n"
     ]
    }
   ],
   "source": [
    "#basicamente el ejercicio 6, lo unico que puedo ajustar n y n_prime del enunciado como quiero\n",
    "#y tengo la funcion que me permite generar los datos que es \n",
    "#la funcion create_parity_data\n",
    "n = 2 #la dimension n de los datos de entrada que vamos a recorrer\n",
    "n_prime = 2 # el n prima del enunciado\n",
    "epocas = 300 #cantidad de epocas a realizar\n",
    "e = range(epocas)\n",
    "lr = 0.05 #learning rate\n",
    "x_train,y_train = create_parity_data(n)\n",
    "mg = 1\n",
    "#hacemos todo lo del ej6 aca\n",
    "model = models.Network()\n",
    "model.add(layers.Dense(n_prime,activations.Tanh(),mg,n))\n",
    "model.add(layers.Dense(1,activations.Tanh(),mg))\n",
    "bs = x_train.shape[0] #batch size para stochastic gradient descendent\n",
    "loss_tr,loss_ts,acc_tr,acc_ts = model.fit(x_train,y_train,None,None,lr,epocas,bs,metrics.accuracy_xor,losses.MSE,optimizers.SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(e,acc_tr) #grafico loss-epocas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(e,loss_tr) #grafico acc-epocas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
