{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comienzo de metrics.py\n",
    "#solo necesito poder evaluar con las metricas\n",
    "#las hago como funciones\n",
    "\n",
    "#ambas reciben los scores de la prediccion (scores) y los scores verdaderos (y)\n",
    "#retorna la metrica correspondiente\n",
    "#chequeado ok ej 8\n",
    "\n",
    "def accuracy(scores,y):\n",
    "    acc = np.mean(scores == y) #calculo accuracy\n",
    "    return acc\n",
    "\n",
    "def MSE(scores,y):\n",
    "    mse = np.mean(np.sum((scores-y)**2,axis=1))\n",
    "    return mse\n",
    "\n",
    "#esta es especifica para el problema del XOR\n",
    "def accuracy_xor(scores,y):\n",
    "    scores[scores>0.9] = 1 #umbral que aconsejaron en clase\n",
    "    scores[scores<-0.9] = -1 #idem \n",
    "    acc = np.mean(scores==y)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comienzo de loss.py\n",
    "#lo codeo aca y despues lo paso a otro archivo\n",
    "#chequeado ok ej8\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,scores,y):\n",
    "        pass\n",
    "    def gradient(self,scores,y):\n",
    "        pass\n",
    "    \n",
    "class MSE(Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def __call__(self,scores,y):\n",
    "        mse = np.mean(np.sum((scores-y)**2,axis=1))\n",
    "        return mse\n",
    "    def gradient(self,scores,y):\n",
    "        gradmse = (scores-y)*2\n",
    "        return gradmse\n",
    "    \n",
    "class CCE(Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def __call__(self,scores,y):\n",
    "        scoresmax = scores.max(axis=1) #maximo de cada fila\n",
    "        scores = scores - scoresmax[:,np.newaxis] #le resto a cada fila el maximo correspondiente\n",
    "        y = np.argmax(y,axis=1)\n",
    "        scores_yi = scores[np.arange(scores.shape[0]),y] #estos son los f_yi\n",
    "        expscores = np.exp(scores) #hace exp(scores)\n",
    "        sum_expscores = expscores.sum(axis=1) #suma de los exp(scores) por fila \n",
    "        loss = np.log(sum_expscores) - scores_yi #aca estan las loss_i en vector fila\n",
    "        loss = loss.mean() #vector fila, solo hago el mean, no sobre algun axis particular\n",
    "        return loss\n",
    "    def gradient(self,scores,y):\n",
    "        y = np.argmax(y,axis=1)\n",
    "        y = y.flatten()\n",
    "        scoresmax = scores.max(axis=1) #maximo de cada fila\n",
    "        scores = scores - scoresmax[:,np.newaxis] #le resto a cada fila el maximo correspondiente\n",
    "        scores_yi = scores[np.arange(scores.shape[0]),y] #estos son los f_yi\n",
    "        expscores = np.exp(scores) #hace exp(scores)\n",
    "        sum_expscores = expscores.sum(axis=1) #suma de los exp(scores) por fila \n",
    "        grad = (1/sum_expscores)[:,np.newaxis]*expscores\n",
    "        grad[np.arange(y.shape[0]),y] = grad[np.arange(y.shape[0]),y] - 1\n",
    "        return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comienzo de activation.py\n",
    "#lo codeo aca y despues lo paso a otro archivo\n",
    "#ok, modificado y corregido ej 8\n",
    "\n",
    "#una estructura similar a la de Loss\n",
    "class Activation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self,x):\n",
    "        pass\n",
    "    def gradient(self,x):\n",
    "        pass\n",
    "\n",
    "class ReLu(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def __call__(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    def gradient(self,x):\n",
    "        res = np.where(x>=0,1,0)\n",
    "        return res\n",
    "\n",
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def __call__(self,x):\n",
    "        return np.tanh(x)\n",
    "    def gradient(self,x):\n",
    "        y = (np.cosh(x))**(-2)\n",
    "        return y\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def __call__(self,x):\n",
    "        sig = (1+np.exp(-x))**(-1)\n",
    "        return sig\n",
    "    def gradient(self,x):\n",
    "        gsig = (np.exp(-x))/((1+np.exp(-x))**2)\n",
    "        return gsig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comienzo de optimizers.py\n",
    "#lo codeo aca y despues lo paso a otro archivo\n",
    "#chequeado ej 8 hasta aca\n",
    "\n",
    "class Optimizer():\n",
    "    def __init__(self,lr):\n",
    "        self.lr = lr #el learning rate\n",
    "        #podria eventualmente haber mas cosas aca\n",
    "    def __call__(self, X, Y, model):\n",
    "        pass\n",
    "    def update_weights(self, W, gradW):\n",
    "        pass\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr, bs):\n",
    "        super().__init__(lr)\n",
    "        self.bs = bs\n",
    "    def randomizeMatrixRows(self,x,y):\n",
    "        #x matriz de imagenes (imagenes x dimension)\n",
    "        #y matriz de scores verdaderos (imagenes x categorias)\n",
    "        indices = np.random.choice(x.shape[0], x.shape[0], replace=False)\n",
    "        x1 = x[indices]\n",
    "        y1 = y[indices]\n",
    "        return x1,y1\n",
    "    def __call__(self, X, Y, model,loss):\n",
    "        nit = int(X.shape[0]/self.bs) #numero de iteraciones\n",
    "        X1,Y1 = self.randomizeMatrixRows(X,Y) #shuffle de las imagenes\n",
    "        for j in range(nit):\n",
    "            X_batch = X1[j*self.bs:(j+1)*self.bs,:] #seleccion de batch\n",
    "            Y_batch = Y1[j*self.bs:(j+1)*self.bs]\n",
    "            scores = model.return_scores(X_batch) #variable auxiliar para el calculo de despues \n",
    "            model.grad = loss.gradient(scores,Y_batch) #esto es para el backwards despues, gradiente del ultimo paso del loss\n",
    "            model.backward(X_batch, Y_batch,self.lr) #backwards de model\n",
    "    def update_weights(self, W, gradW):\n",
    "        W -= self.lr * gradW # SGD, paso de optimizacion despues de calcular gradW\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comienzo de layers.py\n",
    "#lo codeo aca y despues lo paso a otro archivo\n",
    "\n",
    "#chequeado todo aca tambien\n",
    "\n",
    "#base para cualquier tipo de layer/capa\n",
    "class BaseLayer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def set_output_shape(self):\n",
    "        pass\n",
    "    def get_output_shape(self):\n",
    "        pass\n",
    "    \n",
    "#clases para capas especiales sin pesos\n",
    "\n",
    "#clase especial de Layer que es la de Input o entrada\n",
    "class Input(BaseLayer):\n",
    "    def __init__(self,dim_input): #donde dim_xi va a ser la dimension del input de toda la red\n",
    "        super().__init__()\n",
    "        self.dim_input = dim_input\n",
    "    def set_output_shape(self,dim):\n",
    "        self.dim_input = dim\n",
    "    def get_output_shape(self):\n",
    "        return self.dim_input\n",
    "    \n",
    "#capa de concatenacion que me va a servir para realimentacion\n",
    "class Concat(BaseLayer):\n",
    "    def __init__(self,input_layer):\n",
    "        super().__init__()\n",
    "        self.dim_1 = input_layer.get_output_shape()\n",
    "        self.dim_2 = None\n",
    "        self.dim_output = None\n",
    "        #la creo como clase vacia en principio\n",
    "    #concatena los input que le de al llamar al objeto como funcion \n",
    "    def __call__(self,x,y):\n",
    "        #concateno los inputs con los que llamo a la capa de concatenacion\n",
    "        xy_concat = np.hstack((x,y))\n",
    "        return xy_concat\n",
    "    def set_input_shape(self,dim2): #seteo el input shape que va a venir de la neurona anterior\n",
    "        self.dim_2 = dim2\n",
    "    def get_input1_shape(self):\n",
    "        return self.dim_1\n",
    "    def get_input2_shape(self):\n",
    "        return self.dim_2\n",
    "    def get_output_shape(self):\n",
    "        return self.dim_1 + self.dim_2\n",
    "    def set_output_shape(self,dim):\n",
    "        self.dim_output = dim #no entiendo muy bien para que es esto, despues lo veremos\n",
    "    def grad_concat(self,scores):\n",
    "        scores = scores[:,self.get_input1_shape():] #creo que asi esta bien, el bias no se lo pongo\n",
    "        return scores\n",
    "        \n",
    "#clases para capas densas con pesos \n",
    "\n",
    "#clase padre para capas con pesos WLayer\n",
    "class WLayer(BaseLayer):\n",
    "    def __init__(self,n_neuronas,activacion,mg=1,input_dim=0): \n",
    "        #input_dim es opcional, sino lo saco de otra capa\n",
    "        #el resto si lo necesito y es propio de cada capa\n",
    "        super().__init__()\n",
    "        self.output_dim = n_neuronas #numero de neuronas, esto es output_dim\n",
    "        self.activacion = activacion #funcion de activacion de la capa\n",
    "        self.input_dim = input_dim #dimension del problema, no de los ejemplos\n",
    "        self.W = None #estos son los pesos de la capa en cuestion\n",
    "        self.mg = mg\n",
    "    def get_input_shape(self):\n",
    "        return self.input_dim\n",
    "    def set_input_shape(self,input_dim):\n",
    "        self.input_dim = input_dim\n",
    "    def get_output_shape(self):\n",
    "        return self.output_dim\n",
    "    def set_output_shape(self,output_dim):\n",
    "        self.output_dim = output_dim\n",
    "    def init_weights(self): #funcion que inicializa los pesos, idem para todo tipo de cada que podria tener \n",
    "        #self.W = np.zeros(input_dim+1,output_dim) el +1 es por bias, recordatorio de las dimensiones\n",
    "        self.W = np.random.rand(self.input_dim + 1,self.output_dim) * self.mg\n",
    "    def get_weights(self): #devuelve la matriz de pesos\n",
    "        return self.W\n",
    "    def update_weights(self,W_new):\n",
    "        #susceptible a modificaciones\n",
    "        #actualiza los pesos, creo que asi esta ok\n",
    "        self.W = W_new\n",
    "    def addBias(self,x):\n",
    "        aux = np.ones((x.shape[0],1))\n",
    "        x_bias = np.hstack((aux,x))\n",
    "        return x_bias\n",
    "        \n",
    "#clase heredada de WLayer, capa de neuronas densa\n",
    "class Dense(WLayer):\n",
    "    def __init__(self,n_neuronas,activacion,mg=1,input_dim=0): \n",
    "        #herencia de la clase WLayer, es el mismo constructor\n",
    "        super().__init__(n_neuronas,activacion,mg,input_dim)\n",
    "        self.y_i = None\n",
    "    def dot(self,x): #esto devuelve s_i \n",
    "        x_prime = self.addBias(x) #es el x' con el bias aniadido\n",
    "        y = x_prime.dot(self.W) #multiplicacion\n",
    "        self.y_i = np.copy(y)\n",
    "        y = self.activacion(y) #aplica la activacion\n",
    "        return y\n",
    "    def __call__(self,x):\n",
    "        return self.dot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comienzo de models.py\n",
    "#lo codeo aca y despues lo paso a otro archivo\n",
    "\n",
    "class Network():\n",
    "    def __init__(self):\n",
    "        self.list_neuronas = [] #lista de neuronas\n",
    "        self.grad = None #variable auxiliar, despues del fit calculo el gradiente con la loss y despues lo termino en backward\n",
    "        self.optm = None #para poder usarlo en backwards para actualizar los pesos\n",
    "    def add(self,capa_neuronas):\n",
    "        #chequeo si es la primera capa\n",
    "        if (len(self.list_neuronas) != 0):\n",
    "            output_old_neurona = (self.list_neuronas[len(self.list_neuronas)-1]).get_output_shape()\n",
    "            capa_neuronas.set_input_shape(output_old_neurona) #seteo el input de la nueva neurona\n",
    "        if(isinstance(capa_neuronas,Concat)):\n",
    "            pass\n",
    "        else:\n",
    "            capa_neuronas.init_weights() #inicializo\n",
    "        self.list_neuronas.append(capa_neuronas) #aniado neurona a la lista\n",
    "    def get_layer(self,numero_layer): #devuelve la layer dado el numero de la layer que quiero\n",
    "        return self.list_neuronas[numero_layer] #empieza desde la capa 0\n",
    "    def fit (self,x,y,x_test=None,y_test=None,lr = 1e-3,epochs = 100,bs = 100,acc = accuracy_xor, loss_class = MSE,opt_class = SGD):\n",
    "        optm = opt_class(lr,bs) #creo objeto de optimizador, hace loop de batchs\n",
    "        self.optm = optm\n",
    "        x_test = np.array(x_test)\n",
    "        loss = loss_class() #creo objeto loss  \n",
    "        loss_tr = [] #vector de loss de datos de training\n",
    "        loss_ts = [] #vector de loss de datos de testing\n",
    "        acc_tr = [] #vector de accuracy de datos de training\n",
    "        acc_ts = [] #vector de accuracy de datos de testing\n",
    "        for ie in range(epochs):\n",
    "            #forward la primera vez, devuelvo el gradiente de la loss escencialmente\n",
    "            #backward path\n",
    "            optm(x,y,self,loss) #hago call del optimizador, que hace backwards una vez y update de los pesos\n",
    "            #calculo loss y accuracy para los datos de training\n",
    "            scores = self.return_scores(x) #aniado esta funcion que me devuelve los scores solamente despues de actualizar\n",
    "            loss_aux = loss(scores,y) #calculo loss de los datos de training\n",
    "            acc_aux = acc(scores,y) #calculo accuracy de los datos de training\n",
    "            loss_tr.append(loss_aux) #aniado la loss de los de training \n",
    "            acc_tr.append(acc_aux)  #aniado la accuracy de los de training\n",
    "            print(\"training data: \",loss_aux,acc_aux) #printeo loss y acc de training data\n",
    "            #hago lo mismo para los datos de testing basicamente\n",
    "            if(x_test != None):\n",
    "                scores = self.scores(x_test)\n",
    "                loss_aux = loss(scores,y_test)\n",
    "                acc_aux = acc(scores,y_test)\n",
    "                loss_ts.append(loss_aux)\n",
    "                acc_ts.append(acc_aux)\n",
    "                print(\"testing data: \",loss_aux,acc_aux) #printeo loss y acc de testing data\n",
    "        return loss_tr,loss_ts,acc_tr,acc_ts\n",
    "    def forward_upto(self,x_input,j): \n",
    "        #hace forward hasta la capa j-esima \n",
    "        s_i = np.copy(x_input)\n",
    "        for i in range(j+1):\n",
    "            if(isinstance(self.list_neuronas[i],Concat)):\n",
    "                s_i = self.list_neuronas[i](x_input,s_i)\n",
    "            else:\n",
    "                s_i = self.list_neuronas[i](s_i) #estos son los s_i que salen de cada neurona \n",
    "        return s_i #devuelve el resultado de forward up to j-esima capa\n",
    "    def return_scores(self,x): #retorna los scores\n",
    "        scores = self.forward_upto(x,len(self.list_neuronas)-1)\n",
    "        return scores\n",
    "    def predict(self,x): #calcula los scores con la funcion anterior y devuelve la prediccion\n",
    "        scores = self.return_scores(x)\n",
    "        y = np.argmax(scores,axis=1)\n",
    "        return y\n",
    "    def backward(self,x,y,lr):\n",
    "        #le paso self, los datos x,y y lr es el learning rate porque a esta funcion la llama el opt\n",
    "        #y me parece que es la manera mas facil de actualizar los pesos \n",
    "        n_capas = len(self.list_neuronas) #cantidad total de capas totales\n",
    "        grad = self.grad #idem anterior\n",
    "        for j in reversed(range(n_capas)): #recorro hasta la capa 0 de input\n",
    "            capa_actual = self.get_layer(j) #capa actual de trabajo\n",
    "            capa_anterior = self.get_layer(j-1)\n",
    "            s_i_ant = self.forward_upto(x,j-1) #s_i de la capa i-1, capa anterior\n",
    "            if(isinstance(capa_actual,Concat)):\n",
    "                grad = capa_actual.grad_concat(grad)\n",
    "            else:\n",
    "                #sino es una de concatenacion (o sea, es densa basicamente)\n",
    "                grad = grad * capa_actual.activacion.gradient(capa_actual.y_i)\n",
    "                s_i_ant = capa_actual.addBias(s_i_ant)\n",
    "                grad_wi = (s_i_ant.T).dot(grad)\n",
    "                grad = grad.dot(capa_actual.W.T)\n",
    "                grad = grad[:,1:]\n",
    "                capa_actual.W = self.optm.update_weights(capa_actual.W,grad_wi)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#problema XOR 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:  1.5627783055138615 0.0\n",
      "training data:  1.373588434199891 0.0\n",
      "training data:  1.2025956631547117 0.0\n",
      "training data:  1.0793789927122601 0.0\n",
      "training data:  1.0082074511724377 0.0\n",
      "training data:  0.9721061709378208 0.0\n",
      "training data:  0.9514478588021594 0.0\n",
      "training data:  0.9356450795688406 0.0\n",
      "training data:  0.9210492176056178 0.0\n",
      "training data:  0.9067989826479391 0.0\n",
      "training data:  0.8929062729416251 0.0\n",
      "training data:  0.8796065171282558 0.0\n",
      "training data:  0.8671387938170563 0.0\n",
      "training data:  0.8556664712024704 0.0\n",
      "training data:  0.845256717726304 0.0\n",
      "training data:  0.8358910546593512 0.0\n",
      "training data:  0.8274903786765393 0.0\n",
      "training data:  0.8199422799343359 0.0\n",
      "training data:  0.8131233577591197 0.0\n",
      "training data:  0.8069139176163613 0.0\n",
      "training data:  0.8012055733371272 0.0\n",
      "training data:  0.7959036425258195 0.0\n",
      "training data:  0.7909263505613771 0.0\n",
      "training data:  0.7862024149106952 0.0\n",
      "training data:  0.7816680246125043 0.0\n",
      "training data:  0.7772637744349083 0.0\n",
      "training data:  0.7729318141073944 0.0\n",
      "training data:  0.7686133119191983 0.0\n",
      "training data:  0.7642462712592434 0.0\n",
      "training data:  0.7597637447481282 0.0\n",
      "training data:  0.755092539135455 0.0\n",
      "training data:  0.7501525766432448 0.0\n",
      "training data:  0.7448571553117013 0.0\n",
      "training data:  0.7391144030436969 0.0\n",
      "training data:  0.7328302015978367 0.0\n",
      "training data:  0.7259127038979981 0.0\n",
      "training data:  0.7182782126822052 0.0\n",
      "training data:  0.7098575982668582 0.0\n",
      "training data:  0.7006016787383653 0.0\n",
      "training data:  0.6904833083603454 0.0\n",
      "training data:  0.6794937257170153 0.0\n",
      "training data:  0.6676314110768756 0.0\n",
      "training data:  0.6548833981501088 0.0\n",
      "training data:  0.6412012532615554 0.0\n",
      "training data:  0.6264760037271684 0.0\n",
      "training data:  0.6105177154477746 0.0\n",
      "training data:  0.5930467091983287 0.0\n",
      "training data:  0.5737058929730126 0.0\n",
      "training data:  0.5521078495972335 0.0\n",
      "training data:  0.527932228502457 0.0\n",
      "training data:  0.5010761230511933 0.0\n",
      "training data:  0.47181568648735855 0.0\n",
      "training data:  0.4408726013817922 0.0\n",
      "training data:  0.40927744661125964 0.0\n",
      "training data:  0.3780678677963182 0.0\n",
      "training data:  0.3480303690834609 0.0\n",
      "training data:  0.3196424967664139 0.0\n",
      "training data:  0.2931593394351654 0.0\n",
      "training data:  0.2687073244878557 0.0\n",
      "training data:  0.24632985774201763 0.0\n",
      "training data:  0.22600318945314174 0.0\n",
      "training data:  0.20764761229319106 0.0\n",
      "training data:  0.1911419909566339 0.0\n",
      "training data:  0.17633971780632884 0.0\n",
      "training data:  0.163082888174806 0.0\n",
      "training data:  0.1512130757308618 0.0\n",
      "training data:  0.1405785511014243 0.0\n",
      "training data:  0.1310385209231897 0.0\n",
      "training data:  0.12246516601203868 0.0\n",
      "training data:  0.11474419493731217 0.0\n",
      "training data:  0.10777447573299911 0.0\n",
      "training data:  0.10146714885932367 0.0\n",
      "training data:  0.09574449156913656 0.0\n",
      "training data:  0.09053870447714288 0.0\n",
      "training data:  0.08579072175111851 0.0\n",
      "training data:  0.0814491002592104 0.0\n",
      "training data:  0.07746901368642932 0.0\n",
      "training data:  0.07381135978261484 0.0\n",
      "training data:  0.07044197860859354 0.0\n",
      "training data:  0.06733097416830386 0.0\n",
      "training data:  0.06445212930820109 0.0\n",
      "training data:  0.06178240302523569 0.0\n",
      "training data:  0.05930149958195291 0.0\n",
      "training data:  0.0569914996031822 0.0\n",
      "training data:  0.05483654433368355 0.0\n",
      "training data:  0.0528225653007787 0.0\n",
      "training data:  0.050937052656579024 0.0\n",
      "training data:  0.049168856423271366 0.0\n",
      "training data:  0.047508015711959097 0.0\n",
      "training data:  0.04594561172659135 0.0\n",
      "training data:  0.04447364100401634 0.0\n",
      "training data:  0.04308490588792912 0.0\n",
      "training data:  0.04177291969888665 0.0\n",
      "training data:  0.04053182445532081 0.0\n",
      "training data:  0.039356319331683855 0.0\n",
      "training data:  0.038241598318678495 0.0\n",
      "training data:  0.03718329578503386 0.0\n",
      "training data:  0.03617743883748119 0.0\n",
      "training data:  0.03522040554144702 0.0\n",
      "training data:  0.034308888204581195 0.0\n",
      "training data:  0.03343986104284944 0.0\n",
      "training data:  0.032610551648124715 0.0\n",
      "training data:  0.03181841576000676 0.0\n",
      "training data:  0.031061114915487423 0.0\n",
      "training data:  0.030336496610146844 0.0\n",
      "training data:  0.029642576655550704 0.0\n",
      "training data:  0.028977523460874982 0.0\n",
      "training data:  0.028339644003718217 0.0\n",
      "training data:  0.027727371286583834 0.0\n",
      "training data:  0.02713925310246879 0.0\n",
      "training data:  0.02657394195608715 0.0\n",
      "training data:  0.026030186007076263 0.0\n",
      "training data:  0.025506820918577314 0.0\n",
      "training data:  0.025002762509265274 0.0\n",
      "training data:  0.024517000119578195 0.0\n",
      "training data:  0.024048590613854434 0.0\n",
      "training data:  0.023596652949581562 0.0\n",
      "training data:  0.023160363253199923 0.0\n",
      "training data:  0.022738950349067432 0.0\n",
      "training data:  0.022331691694430986 0.0\n",
      "training data:  0.02193790967869205 0.0\n",
      "training data:  0.02155696825001085 0.0\n",
      "training data:  0.021188269836456344 0.0\n",
      "training data:  0.020831252532558884 0.0\n",
      "training data:  0.020485387525328726 0.0\n",
      "training data:  0.02015017673662156 0.0\n",
      "training data:  0.019825150661216717 0.0\n",
      "training data:  0.019509866382163742 0.0\n",
      "training data:  0.019203905746889415 0.0\n",
      "training data:  0.0189068736892698 0.0\n",
      "training data:  0.018618396684389627 0.0\n",
      "training data:  0.018338121324058345 0.0\n",
      "training data:  0.018065713002348292 0.0\n",
      "training data:  0.017800854701485732 0.0\n",
      "training data:  0.0175432458693738 0.0\n",
      "training data:  0.0172926013808727 0.0\n",
      "training data:  0.017048650575718458 0.0\n",
      "training data:  0.016811136366637416 0.0\n",
      "training data:  0.01657981441181944 0.0\n",
      "training data:  0.016354452346454808 0.0\n",
      "training data:  0.01613482906852759 0.0\n",
      "training data:  0.0159207340744956 0.0\n",
      "training data:  0.015711966840881057 0.0\n",
      "training data:  0.015508336248150595 0.0\n",
      "training data:  0.015309660043583256 0.0\n",
      "training data:  0.015115764340113311 0.0\n",
      "training data:  0.014926483148396254 0.0\n",
      "training data:  0.014741657939581508 0.0\n",
      "training data:  0.014561137236488995 0.0\n",
      "training data:  0.014384776231081087 0.0\n",
      "training data:  0.014212436426295673 0.25\n",
      "training data:  0.014043985300466959 0.25\n",
      "training data:  0.0138792959927041 0.5\n",
      "training data:  0.013718247007730971 0.5\n",
      "training data:  0.0135607219388093 0.5\n",
      "training data:  0.013406609207477486 0.5\n",
      "training data:  0.013255801818936727 0.5\n",
      "training data:  0.013108197132007705 0.5\n",
      "training data:  0.012963696642663832 0.5\n",
      "training data:  0.012822205780223472 0.5\n",
      "training data:  0.012683633715353079 0.5\n",
      "training data:  0.012547893179097328 0.5\n",
      "training data:  0.01241490029221006 0.5\n",
      "training data:  0.012284574404114653 0.5\n",
      "training data:  0.012156837940870616 0.5\n",
      "training data:  0.01203161626156923 0.5\n",
      "training data:  0.011908837522622272 0.5\n",
      "training data:  0.011788432549446356 0.5\n",
      "training data:  0.0116703347150806 0.5\n",
      "training data:  0.011554479825308049 0.5\n",
      "training data:  0.0114408060098807 0.5\n",
      "training data:  0.011329253619476435 0.5\n",
      "training data:  0.011219765128040893 0.5\n",
      "training data:  0.011112285040191544 0.5\n",
      "training data:  0.011006759803382567 0.5\n",
      "training data:  0.010903137724549592 0.5\n",
      "training data:  0.010801368890971977 0.5\n",
      "training data:  0.010701405095107546 0.5\n",
      "training data:  0.010603199763170668 0.5\n",
      "training data:  0.010506707887239775 0.5\n",
      "training data:  0.010411885960693746 0.5\n",
      "training data:  0.010318691916789639 0.5\n",
      "training data:  0.010227085070206412 0.5\n",
      "training data:  0.010137026061389637 0.5\n",
      "training data:  0.010048476803543318 0.5\n",
      "training data:  0.009961400432123827 0.5\n",
      "training data:  0.009875761256700413 0.5\n",
      "training data:  0.00979152471505469 0.5\n",
      "training data:  0.009708657329399193 0.5\n",
      "training data:  0.00962712666460297 0.5\n",
      "training data:  0.009546901288317845 0.5\n",
      "training data:  0.009467950732906355 0.5\n",
      "training data:  0.009390245459077363 0.5\n",
      "training data:  0.009313756821141505 0.5\n",
      "training data:  0.009238457033803304 0.5\n",
      "training data:  0.009164319140411822 0.5\n",
      "training data:  0.009091316982596031 0.5\n",
      "training data:  0.00901942517121555 0.5\n",
      "training data:  0.008948619058561173 0.5\n",
      "training data:  0.008878874711742979 0.5\n",
      "training data:  0.008810168887208187 0.5\n",
      "training data:  0.008742479006332946 0.5\n",
      "training data:  0.008675783132036406 0.5\n",
      "training data:  0.008610059946367564 0.5\n",
      "training data:  0.008545288729018276 0.5\n",
      "training data:  0.00848144933671844 0.5\n",
      "training data:  0.008418522183471628 0.5\n",
      "training data:  0.008356488221591666 0.5\n",
      "training data:  0.008295328923502696 0.5\n",
      "training data:  0.008235026264267505 0.5\n",
      "training data:  0.008175562704810336 0.75\n",
      "training data:  0.008116921175802544 0.75\n",
      "training data:  0.008059085062180758 0.75\n",
      "training data:  0.008002038188269188 0.75\n",
      "training data:  0.007945764803478549 0.75\n",
      "training data:  0.007890249568556279 0.75\n",
      "training data:  0.007835477542363122 0.75\n",
      "training data:  0.007781434169153254 0.75\n",
      "training data:  0.007728105266335453 0.75\n",
      "training data:  0.007675477012694792 0.75\n",
      "training data:  0.007623535937054356 0.75\n",
      "training data:  0.007572268907358564 0.75\n",
      "training data:  0.007521663120159538 0.75\n",
      "training data:  0.0074717060904897605 0.75\n",
      "training data:  0.007422385642104422 0.75\n",
      "training data:  0.0073736898980778145 0.75\n",
      "training data:  0.00732560727173937 0.75\n",
      "training data:  0.007278126457934629 1.0\n",
      "training data:  0.007231236424598246 1.0\n",
      "training data:  0.007184926404625675 1.0\n",
      "training data:  0.007139185888031836 1.0\n",
      "training data:  0.007094004614384581 1.0\n",
      "training data:  0.0070493725655022405 1.0\n",
      "training data:  0.007005279958404368 1.0\n",
      "training data:  0.006961717238505696 1.0\n",
      "training data:  0.006918675073043526 1.0\n",
      "training data:  0.00687614434472937 1.0\n",
      "training data:  0.006834116145616012 1.0\n",
      "training data:  0.006792581771171505 1.0\n",
      "training data:  0.0067515327145521295 1.0\n",
      "training data:  0.006710960661066483 1.0\n",
      "training data:  0.006670857482823503 1.0\n",
      "training data:  0.0066312152335571625 1.0\n",
      "training data:  0.0065920261436213145 1.0\n",
      "training data:  0.006553282615148075 1.0\n",
      "training data:  0.006514977217363643 1.0\n",
      "training data:  0.006477102682055647 1.0\n",
      "training data:  0.006439651899186343 1.0\n",
      "training data:  0.006402617912646201 1.0\n",
      "training data:  0.0063659939161428486 1.0\n",
      "training data:  0.006329773249220148 1.0\n",
      "training data:  0.006293949393402888 1.0\n",
      "training data:  0.006258515968462329 1.0\n",
      "training data:  0.006223466728798369 1.0\n",
      "training data:  0.006188795559934011 1.0\n",
      "training data:  0.006154496475118185 1.0\n",
      "training data:  0.006120563612032977 1.0\n",
      "training data:  0.0060869912296016866 1.0\n",
      "training data:  0.006053773704893949 1.0\n",
      "training data:  0.006020905530124776 1.0\n",
      "training data:  0.005988381309743943 1.0\n",
      "training data:  0.0059561957576127625 1.0\n",
      "training data:  0.0059243436942651925 1.0\n",
      "training data:  0.005892820044250303 1.0\n",
      "training data:  0.005861619833553315 1.0\n",
      "training data:  0.005830738187092643 1.0\n",
      "training data:  0.0058001703262902 1.0\n",
      "training data:  0.005769911566712556 1.0\n",
      "training data:  0.005739957315780596 1.0\n",
      "training data:  0.005710303070545259 1.0\n",
      "training data:  0.005680944415527408 1.0\n",
      "training data:  0.00565187702061928 1.0\n",
      "training data:  0.005623096639045989 1.0\n",
      "training data:  0.005594599105384596 1.0\n",
      "training data:  0.005566380333639336 1.0\n",
      "training data:  0.005538436315370714 1.0\n",
      "training data:  0.005510763117877111 1.0\n",
      "training data:  0.005483356882427003 1.0\n",
      "training data:  0.005456213822540049 1.0\n",
      "training data:  0.0054293302223159 1.0\n",
      "training data:  0.005402702434808644 1.0\n",
      "training data:  0.005376326880445933 1.0\n",
      "training data:  0.005350200045491076 1.0\n",
      "training data:  0.0053243184805468425 1.0\n",
      "training data:  0.005298678799099714 1.0\n",
      "training data:  0.005273277676103216 1.0\n",
      "training data:  0.0052481118465992314 1.0\n",
      "training data:  0.005223178104376001 1.0\n",
      "training data:  0.00519847330066174 1.0\n",
      "training data:  0.005173994342852881 1.0\n",
      "training data:  0.005149738193275643 1.0\n",
      "training data:  0.005125701867980099 1.0\n",
      "training data:  0.005101882435565778 1.0\n",
      "training data:  0.005078277016037674 1.0\n",
      "training data:  0.005054882779691906 1.0\n",
      "training data:  0.0050316969460301045 1.0\n",
      "training data:  0.0050087167827016395 1.0\n",
      "training data:  0.004985939604472856 1.0\n",
      "training data:  0.004963362772222542 1.0\n",
      "training data:  0.004940983691962917 1.0\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "x_train = np.array([[-1,-1],[-1,1],[1,-1],[1,1]])\n",
    "y_train = np.array([[1],[-1],[-1],[1]])\n",
    "model = Network() #creo una network\n",
    "mg=1\n",
    "layer_1 = Dense(2,Tanh(),mg,x_train.shape[1]) #capa 1\n",
    "model.add(layer_1) #aniado la primera capa a la red\n",
    "layer_2 = Dense(1,Tanh(),mg) #creo la capa 2\n",
    "model.add(layer_2) #aniado la segunda capa a la red\n",
    "epocas = 300 #cantidad de epocas a realizar\n",
    "lr = 0.05 #learning rate\n",
    "bs = x_train.shape[0] #batch size para stochastic gradient descendent\n",
    "#def fit (self,x,y,x_test=None,y_test=None,lr = 1e-3,epochs = 100,bs = 100,acc = accuracy_xor, loss_class = MSE,opt_class = SGD):\n",
    "loss_tr,loss_ts,acc_tr,acc_ts = model.fit(x_train,y_train,None,None,lr,epocas,bs,accuracy_xor,MSE,SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x23f62772f88>,\n",
       "  <matplotlib.axis.YTick at 0x23f6276ffc8>,\n",
       "  <matplotlib.axis.YTick at 0x23f62767b08>,\n",
       "  <matplotlib.axis.YTick at 0x23f62798ec8>,\n",
       "  <matplotlib.axis.YTick at 0x23f6279c9c8>],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "e = range(epocas)\n",
    "plt.plot(e,acc_tr)\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.yticks(np.arange(0, 1.25, step=0.25))\n",
    "#plt.savefig('ej6sit1_accepoch0p5.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(e,loss_tr)\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss')\n",
    "#plt.savefig('ej6sit1_lossepoch0p5.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#problema 6 XOR 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:  1.7778093273467979 0.25\n",
      "training data:  1.4819772273318095 0.0\n",
      "training data:  1.230511124006487 0.0\n",
      "training data:  1.083144881628177 0.0\n",
      "training data:  1.0189966520591676 0.0\n",
      "training data:  0.9824782913490543 0.0\n",
      "training data:  0.9579579342983093 0.0\n",
      "training data:  0.9396716745415724 0.0\n",
      "training data:  0.9215819344352274 0.0\n",
      "training data:  0.9008910106697741 0.0\n",
      "training data:  0.8764924353235636 0.0\n",
      "training data:  0.8477480354958982 0.0\n",
      "training data:  0.8142649784460101 0.0\n",
      "training data:  0.7759676050971189 0.0\n",
      "training data:  0.7332083343749981 0.0\n",
      "training data:  0.6868137793914859 0.0\n",
      "training data:  0.6380109964779731 0.0\n",
      "training data:  0.5882438253263099 0.0\n",
      "training data:  0.5389495389783601 0.0\n",
      "training data:  0.49137418849179393 0.0\n",
      "training data:  0.4464667233287133 0.0\n",
      "training data:  0.40485020989181236 0.0\n",
      "training data:  0.3668478242076383 0.0\n",
      "training data:  0.33253856490345635 0.0\n",
      "training data:  0.30182265836524724 0.0\n",
      "training data:  0.2744834116879389 0.0\n",
      "training data:  0.2502384810554131 0.0\n",
      "training data:  0.2287782679115411 0.0\n",
      "training data:  0.20979216447724638 0.0\n",
      "training data:  0.19298483559684543 0.0\n",
      "training data:  0.17808509866469352 0.0\n",
      "training data:  0.16484972813751037 0.0\n",
      "training data:  0.15306403130311774 0.0\n",
      "training data:  0.14254053563754718 0.0\n",
      "training data:  0.13311669617373142 0.0\n",
      "training data:  0.12465220273023728 0.0\n",
      "training data:  0.1170262349849839 0.0\n",
      "training data:  0.1101348587196373 0.0\n",
      "training data:  0.1038886582208497 0.0\n",
      "training data:  0.09821064017473727 0.0\n",
      "training data:  0.09303441014951001 0.0\n",
      "training data:  0.08830260458397388 0.0\n",
      "training data:  0.08396555281313185 0.0\n",
      "training data:  0.07998014100097973 0.0\n",
      "training data:  0.07630885035847933 0.0\n",
      "training data:  0.07291894413978324 0.0\n",
      "training data:  0.06978178070793813 0.0\n",
      "training data:  0.066872232917494 0.0\n",
      "training data:  0.06416819689494084 0.0\n",
      "training data:  0.06165017587364244 0.0\n",
      "training data:  0.05930092700718276 0.0\n",
      "training data:  0.05710516103963996 0.0\n",
      "training data:  0.05504928637313367 0.0\n",
      "training data:  0.05312119047268969 0.0\n",
      "training data:  0.051310052719941664 0.0\n",
      "training data:  0.04960618380357426 0.0\n",
      "training data:  0.04800088754608147 0.0\n",
      "training data:  0.046486341740194934 0.0\n",
      "training data:  0.0450554951273346 0.0\n",
      "training data:  0.04370197811425552 0.0\n",
      "training data:  0.04242002520915095 0.0\n",
      "training data:  0.041204407478511895 0.0\n",
      "training data:  0.04005037359237103 0.0\n",
      "training data:  0.03895359824753498 0.0\n",
      "training data:  0.03791013694373501 0.0\n",
      "training data:  0.03691638624264297 0.0\n",
      "training data:  0.035969048769618615 0.0\n",
      "training data:  0.03506510232715914 0.0\n",
      "training data:  0.03420177258084255 0.0\n",
      "training data:  0.03337650885598716 0.0\n",
      "training data:  0.03258696264869702 0.0\n",
      "training data:  0.03183096851038843 0.0\n",
      "training data:  0.031106527011941887 0.0\n",
      "training data:  0.030411789533641237 0.0\n",
      "training data:  0.029745044661175488 0.0\n",
      "training data:  0.029104705997117362 0.0\n",
      "training data:  0.028489301222237175 0.0\n",
      "training data:  0.027897462262404492 0.0\n",
      "training data:  0.027327916435221804 0.0\n",
      "training data:  0.02677947846636893 0.0\n",
      "training data:  0.02625104327930577 0.0\n",
      "training data:  0.02574157947379444 0.0\n",
      "training data:  0.025250123418938706 0.0\n",
      "training data:  0.024775773895319004 0.0\n",
      "training data:  0.02431768722852569 0.0\n",
      "training data:  0.023875072863117835 0.0\n",
      "training data:  0.023447189331905142 0.0\n",
      "training data:  0.02303334058057873 0.0\n",
      "training data:  0.022632872612208622 0.0\n",
      "training data:  0.02224517042006341 0.0\n",
      "training data:  0.0218696551806675 0.0\n",
      "training data:  0.02150578168205449 0.0\n",
      "training data:  0.021153035964857726 0.0\n",
      "training data:  0.020810933156244944 0.0\n",
      "training data:  0.020479015478796825 0.0\n",
      "training data:  0.020156850418279908 0.0\n",
      "training data:  0.019844029035906048 0.0\n",
      "training data:  0.01954016441212723 0.0\n",
      "training data:  0.019244890210309253 0.0\n",
      "training data:  0.018957859349780944 0.0\n",
      "training data:  0.018678742778782176 0.0\n",
      "training data:  0.0184072283387511 0.0\n",
      "training data:  0.018143019712209642 0.0\n",
      "training data:  0.0178858354472401 0.0\n",
      "training data:  0.01763540805220081 0.0\n",
      "training data:  0.01739148315491884 0.0\n",
      "training data:  0.017153818721125282 0.0\n",
      "training data:  0.016922184327374565 0.0\n",
      "training data:  0.016696360484116497 0.0\n",
      "training data:  0.01647613800497506 0.0\n",
      "training data:  0.01626131741863534 0.0\n",
      "training data:  0.016051708420054198 0.0\n",
      "training data:  0.015847129357992925 0.0\n",
      "training data:  0.01564740675612763 0.0\n",
      "training data:  0.015452374865224688 0.0\n",
      "training data:  0.01526187524407982 0.0\n",
      "training data:  0.015075756367109738 0.0\n",
      "training data:  0.014893873256659838 0.0\n",
      "training data:  0.014716087138248204 0.0\n",
      "training data:  0.014542265117110941 0.0\n",
      "training data:  0.014372279874543474 0.0\n",
      "training data:  0.014206009382652688 0.25\n",
      "training data:  0.014043336636242873 0.25\n",
      "training data:  0.013884149400658522 0.25\n",
      "training data:  0.013728339974496705 0.25\n",
      "training data:  0.013575804966186353 0.5\n",
      "training data:  0.0134264450835061 0.5\n",
      "training data:  0.013280164935183632 0.5\n",
      "training data:  0.013136872843781955 0.5\n",
      "training data:  0.012996480669137658 0.5\n",
      "training data:  0.012858903641669377 0.5\n",
      "training data:  0.012724060204924588 0.5\n",
      "training data:  0.012591871866777989 0.5\n",
      "training data:  0.01246226305873675 0.5\n",
      "training data:  0.012335161002846672 0.5\n",
      "training data:  0.012210495585728263 0.5\n",
      "training data:  0.012088199239305233 0.5\n",
      "training data:  0.01196820682781731 0.5\n",
      "training data:  0.01185045554073824 0.5\n",
      "training data:  0.011734884791244522 0.5\n",
      "training data:  0.011621436119905128 0.5\n",
      "training data:  0.011510053103284257 0.5\n",
      "training data:  0.01140068126716942 0.5\n",
      "training data:  0.011293268004156356 0.5\n",
      "training data:  0.011187762495339901 0.5\n",
      "training data:  0.011084115635875667 0.5\n",
      "training data:  0.010982279964193362 0.5\n",
      "training data:  0.010882209594655703 0.5\n",
      "training data:  0.010783860153470746 0.5\n",
      "training data:  0.010687188717676753 0.5\n",
      "training data:  0.010592153757030818 0.5\n",
      "training data:  0.01049871507864232 0.5\n",
      "training data:  0.010406833774202246 0.5\n",
      "training data:  0.01031647216966888 0.5\n",
      "training data:  0.010227593777278072 0.5\n",
      "training data:  0.01014016324975506 0.5\n",
      "training data:  0.010054146336611542 0.5\n",
      "training data:  0.009969509842419108 0.5\n",
      "training data:  0.00988622158695601 0.5\n",
      "training data:  0.00980425036713085 0.5\n",
      "training data:  0.009723565920592009 0.5\n",
      "training data:  0.009644138890937074 0.5\n",
      "training data:  0.009565940794441223 0.5\n",
      "training data:  0.009488943988228594 0.5\n",
      "training data:  0.009413121639814368 0.5\n",
      "training data:  0.009338447697949902 0.5\n",
      "training data:  0.00926489686470668 0.5\n",
      "training data:  0.009192444568738592 0.5\n",
      "training data:  0.009121066939665263 0.5\n",
      "training data:  0.009050740783522452 0.5\n",
      "training data:  0.008981443559228313 0.5\n",
      "training data:  0.008913153356017063 0.5\n",
      "training data:  0.008845848871794595 0.5\n",
      "training data:  0.008779509392372296 0.5\n",
      "training data:  0.008714114771538417 0.5\n",
      "training data:  0.008649645411927764 0.5\n",
      "training data:  0.008586082246653295 0.5\n",
      "training data:  0.008523406721664363 0.5\n",
      "training data:  0.008461600778798598 0.5\n",
      "training data:  0.00840064683949625 0.5\n",
      "training data:  0.008340527789146696 0.5\n",
      "training data:  0.008281226962039437 0.5\n",
      "training data:  0.008222728126892244 0.5\n",
      "training data:  0.008165015472931188 0.5\n",
      "training data:  0.008108073596498247 0.5\n",
      "training data:  0.008051887488163514 0.5\n",
      "training data:  0.007996442520319946 0.5\n",
      "training data:  0.0079417244352401 0.5\n",
      "training data:  0.007887719333574834 0.5\n",
      "training data:  0.007834413663275135 0.5\n",
      "training data:  0.007781794208919298 0.75\n",
      "training data:  0.0077298480814281395 1.0\n",
      "training data:  0.007678562708152231 1.0\n",
      "training data:  0.007627925823315429 1.0\n",
      "training data:  0.007577925458800053 1.0\n",
      "training data:  0.007528549935259703 1.0\n",
      "training data:  0.007479787853546064 1.0\n",
      "training data:  0.00743162808643714 1.0\n",
      "training data:  0.007384059770654592 1.0\n",
      "training data:  0.0073370722991585305 1.0\n",
      "training data:  0.007290655313708731 1.0\n",
      "training data:  0.0072447986976814475 1.0\n",
      "training data:  0.007199492569131997 1.0\n",
      "training data:  0.007154727274093104 1.0\n",
      "training data:  0.007110493380100018 1.0\n",
      "training data:  0.0070667816699334935 1.0\n",
      "training data:  0.007023583135571929 1.0\n",
      "training data:  0.006980888972345082 1.0\n",
      "training data:  0.006938690573281122 1.0\n",
      "training data:  0.006896979523640027 1.0\n",
      "training data:  0.006855747595625934 1.0\n",
      "training data:  0.006814986743271974 1.0\n",
      "training data:  0.006774689097490775 1.0\n",
      "training data:  0.006734846961284826 1.0\n",
      "training data:  0.006695452805110381 1.0\n",
      "training data:  0.006656499262389421 1.0\n",
      "training data:  0.006617979125164283 1.0\n",
      "training data:  0.0065798853398894575 1.0\n",
      "training data:  0.0065422110033558165 1.0\n",
      "training data:  0.006504949358742414 1.0\n",
      "training data:  0.006468093791791058 1.0\n",
      "training data:  0.006431637827099542 1.0\n",
      "training data:  0.006395575124529061 1.0\n",
      "training data:  0.006359899475721818 1.0\n",
      "training data:  0.006324604800725024 1.0\n",
      "training data:  0.006289685144717322 1.0\n",
      "training data:  0.006255134674834284 1.0\n",
      "training data:  0.006220947677089342 1.0\n",
      "training data:  0.0061871185533869475 1.0\n",
      "training data:  0.006153641818624718 1.0\n",
      "training data:  0.006120512097881487 1.0\n",
      "training data:  0.006087724123688431 1.0\n",
      "training data:  0.006055272733380276 1.0\n",
      "training data:  0.006023152866524058 1.0\n",
      "training data:  0.005991359562422537 1.0\n",
      "training data:  0.005959887957690188 1.0\n",
      "training data:  0.005928733283898853 1.0\n",
      "training data:  0.00589789086529106 1.0\n",
      "training data:  0.005867356116558747 1.0\n",
      "training data:  0.005837124540685004 1.0\n",
      "training data:  0.005807191726847045 1.0\n",
      "training data:  0.005777553348378248 1.0\n",
      "training data:  0.0057482051607873055 1.0\n",
      "training data:  0.005719142999832812 1.0\n",
      "training data:  0.005690362779651311 1.0\n",
      "training data:  0.005661860490937201 1.0\n",
      "training data:  0.0056336321991727325 1.0\n",
      "training data:  0.005605674042906738 1.0\n",
      "training data:  0.005577982232080163 1.0\n",
      "training data:  0.0055505530463973085 1.0\n",
      "training data:  0.005523382833741163 1.0\n",
      "training data:  0.0054964680086313274 1.0\n",
      "training data:  0.005469805050723534 1.0\n",
      "training data:  0.005443390503349145 1.0\n",
      "training data:  0.005417220972093606 1.0\n",
      "training data:  0.0053912931234124505 1.0\n",
      "training data:  0.005365603683284017 1.0\n",
      "training data:  0.005340149435897424 1.0\n",
      "training data:  0.005314927222374971 1.0\n",
      "training data:  0.005289933939527743 1.0\n",
      "training data:  0.005265166538643628 1.0\n",
      "training data:  0.005240622024306538 1.0\n",
      "training data:  0.00521629745324613 1.0\n",
      "training data:  0.005192189933216961 1.0\n",
      "training data:  0.005168296621906163 1.0\n",
      "training data:  0.00514461472586911 1.0\n",
      "training data:  0.005121141499491788 1.0\n",
      "training data:  0.005097874243979511 1.0\n",
      "training data:  0.005074810306370921 1.0\n",
      "training data:  0.005051947078576724 1.0\n",
      "training data:  0.005029281996442327 1.0\n",
      "training data:  0.0050068125388337565 1.0\n",
      "training data:  0.004984536226746182 1.0\n",
      "training data:  0.004962450622434328 1.0\n",
      "training data:  0.004940553328564287 1.0\n",
      "training data:  0.004918841987385984 1.0\n",
      "training data:  0.0048973142799257794 1.0\n",
      "training data:  0.0048759679251987095 1.0\n",
      "training data:  0.004854800679439658 1.0\n",
      "training data:  0.0048338103353530275 1.0\n",
      "training data:  0.004812994721380425 1.0\n",
      "training data:  0.004792351700985779 1.0\n",
      "training data:  0.004771879171957481 1.0\n",
      "training data:  0.0047515750657270315 1.0\n",
      "training data:  0.004731437346703744 1.0\n",
      "training data:  0.004711464011625139 1.0\n",
      "training data:  0.004691653088922472 1.0\n",
      "training data:  0.0046720026381011385 1.0\n",
      "training data:  0.0046525107491353565 1.0\n",
      "training data:  0.004633175541877019 1.0\n",
      "training data:  0.004613995165478068 1.0\n",
      "training data:  0.004594967797826157 1.0\n",
      "training data:  0.004576091644993259 1.0\n",
      "training data:  0.004557364940696838 1.0\n",
      "training data:  0.004538785945773256 1.0\n",
      "training data:  0.004520352947663149 1.0\n",
      "training data:  0.004502064259908329 1.0\n",
      "training data:  0.004483918221660076 1.0\n",
      "training data:  0.004465913197198376 1.0\n",
      "training data:  0.004448047575461956 1.0\n"
     ]
    }
   ],
   "source": [
    "#Dataset\n",
    "x_train = np.array([[-1,-1],[-1,1],[1,-1],[1,1]])\n",
    "y_train = np.array([[1],[-1],[-1],[1]])\n",
    "\n",
    "input_layer = Input(x_train.shape[1]) #input layer, la creamos para crear la concat layer despues\n",
    "layer_1 = Dense(2,Tanh(),mg,x_train.shape[1]) #capa 1\n",
    "layer_2 = Dense(1,Tanh(),mg) #creo la capa 2\n",
    "\n",
    "model = Network() #creo objeto tipo network\n",
    "\n",
    "model.add(layer_1) #aniado la primera capa a la red\n",
    "model.add(Concat(input_layer))\n",
    "model.add(layer_2) #aniado la segunda capa a la red\n",
    "\n",
    "epocas = 300 #cantidad de epocas a realizar\n",
    "lr = 0.05 #learning rate\n",
    "bs = x_train.shape[0] #batch size para stochastic gradient descendent\n",
    "\n",
    "\n",
    "loss_tr,loss_ts,acc_tr,acc_ts = model.fit(x_train,y_train,None,None,lr,epocas,bs,accuracy_xor,MSE,SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.YTick at 0x23f626a0988>,\n",
       "  <matplotlib.axis.YTick at 0x23f62694188>,\n",
       "  <matplotlib.axis.YTick at 0x23f6268f9c8>,\n",
       "  <matplotlib.axis.YTick at 0x23f626c7648>,\n",
       "  <matplotlib.axis.YTick at 0x23f626cb1c8>],\n",
       " [Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, ''),\n",
       "  Text(0, 0, '')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEICAYAAABF82P+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZX0lEQVR4nO3da5Bk5XnY8f8zl+UuY2kxJizLYmllmShCwlNIWC50cYSAiBDKcQniBJVke4MMqlgpuYLjiizlS2KpnKQUYa1XDrqkkNYXhEQqK4FCbMmxbrsgLgsIa1ljWC3mIhAYYTF9efLhnJ7pnTk9c2bZMz3T/f9VTW33ufQ+p857+un3ct4TmYkkSQtNDDsASdLaZIKQJFUyQUiSKpkgJEmVTBCSpEpTww7gSNq4cWNu2bJl2GFI0rpx2223PZGZJ1WtG6kEsWXLFvbs2TPsMCRp3YiIvxm0ziYmSVIlE4QkqZIJQpJUyQQhSapkgpAkVWosQUTEdRHxWETsHbA+IuIjEbEvIu6KiLP71l0QEfeX665pKkZJ0mBN1iA+CVywxPoLga3l3zbgYwARMQlcW64/E7g8Is5sME5JUoXG7oPIzK9GxJYlNrkE+HQW841/IyJOjIhTgC3AvszcDxARO8tt720qVknw8JPP8ae3HcBHAKw/xx41xZVveOkR/9xh3ih3KvBw3/sD5bKq5a8d9CERsY2iBsLmzZuPfJTSmLj+mw+x/SsPEDHsSLRSG48/auQSRFUxzCWWV8rMHcAOgJmZGX/6SIfp+XaHE46a4u4PvnXYoWiNGGaCOACc1vd+E3AQ2DBguaQGtTvJ1KTVB80b5jDXm4ArytFMrwOezsxHgN3A1og4IyI2AJeV20pqULvbZWrSke+a11gNIiI+C7wR2BgRB4DfAaYBMnM7sAu4CNgHPAe8s1zXjoirgZuBSeC6zLynqTglFVqdZIMJQn2aHMV0+TLrE7hqwLpdFAlE0ippdbo2MekQ/lyQBJR9EBMmCM0zQUgCihrEtE1M6mNpkARAu5smCB3C0iAJsA9Ci5kgJAFlE9OEXwmaZ2mQBBSd1NNT1iA0zwQhCSibmKxBqI+lQRJQ3Cg3bR+E+pggJAHlVBvWINTH0iAJ6PVB+JWgeZYGSQDMdrpMeye1+pggJAFO963FTBCSAKf71mKWBkmA031rMUuDJKB3H4RNTJpngpAE9Pog/ErQPEuDJDKTVrfrjXI6hAlCEp1ukonTfesQlgZJtLsJ4DBXHcIEIYlWpwvgdN86hKVBEu1OUYOwD0L9TBCS5moQjmJSP0uDJFpdaxBazAQhiXavBmEfhPpYGiTR6vVBON23+lgaJPWNYrKJSfNMEJLmRjHZSa1+lgZJtLq9UUzWIDTPBCFprgbhdN/qZ2mQNH8fhH0Q6mOCkOSNcqpkaZBkE5MqWRok9dUgbGLSPBOEJKfaUCUThCSn2lAlS4Ok+em+nWpDfSwNkph1qg1VMEFImm9ichST+lgaJPlMalUyQUiam+7b+yDUz9Igyak2VKnRBBERF0TE/RGxLyKuqVj/mxFxR/m3NyI6EfHict2DEXF3uW5Pk3FK467XBzFpglCfqaY+OCImgWuBtwAHgN0RcVNm3tvbJjM/DHy43P5i4L2Z+WTfx7wpM59oKkZJhVY32TA5QYQJQvOarEGcA+zLzP2ZOQvsBC5ZYvvLgc82GI+kAVrtrh3UWqTJBHEq8HDf+wPlskUi4ljgAuCGvsUJ3BIRt0XEtkH/SURsi4g9EbHn8ccfPwJhS+On3U37H7RIkwmiqrTlgG0vBv5yQfPS6zPzbOBC4KqIOK9qx8zckZkzmTlz0kknvbCIpTHV6nSZdgSTFmiyRBwATut7vwk4OGDby1jQvJSZB8t/HwNupGiyktSAdidNEFqkyRKxG9gaEWdExAaKJHDTwo0i4seANwBf6Ft2XESc0HsNnA/sbTBWaay1OvZBaLHGRjFlZjsirgZuBiaB6zLznoi4sly/vdz0UuCWzPxh3+4nAzeWIyqmgM9k5peailUad62uNQgt1liCAMjMXcCuBcu2L3j/SeCTC5btB85qMjZJ89qdrp3UWsSfDJJo2QehCpYISeUoJmsQOpQJQhLtbtepvrWIJUISrY43ymkxE4QkWp0uG3zcqBawREiibQ1CFUwQksob5fw60KEsEZJol9N9S/0sEZKcakOVTBCSyj4Ivw50KEuEJG+UUyUThCTaTtanCpYIST5yVJVMEJJodX2inBazREjyRjlVMkFIYy4z7YNQJUuENOZanQRwFJMWMUFIY67d7QI41YYWsURIY26+BuHXgQ5liZDGXKtT1CBsYtJCJghpzLXLGoRTbWghS4Q05no1CG+U00ImCGnMtbtFDcLpvrXQsiUiIt4WEZYcaURZg9Agdb74LwO+GxEfioifaTogSatrLkHYB6EFli0RmfkvgdcADwCfiIivR8S2iDih8egkNa7tjXIaoNZPhsx8BrgB2AmcAlwK3B4R72kwNkmroHejnPdBaKE6fRAXR8SNwP8FpoFzMvNC4CzgfQ3HJ6lhs+1ymKs1CC0wVWObXwL+a2Z+tX9hZj4XEe9qJixJq8UahAapkyB+B3ik9yYijgFOzswHM/PWxiKTtCrmb5SzBqFD1fnJ8CdAt+99p1wmaQTMdqxBqFqdEjGVmbO9N+XrDc2FJGk1tZ2sTwPUKRGPR8Q/7b2JiEuAJ5oLSdJqmp/u2yYmHapOH8SVwPUR8VEggIeBKxqNStKq6U337VQbWmjZBJGZDwCvi4jjgcjMv2s+LEmrxak2NEidGgQR8U+AfwgcHVEUosz8jw3GJWmVtJ1qQwPUuVFuO/B24D0UTUy/BJzecFySVonPpNYgdX4y/FxmXgE8lZkfBM4FTms2LEmrxRvlNEidEvGj8t/nIuIfAC3gjOZCkrSaejUI+yC0UJ0+iP8VEScCHwZuBxL4eJNBSVo9c8+ktg9CCyxZIsoHBd2amT/IzBso+h5ekZnvr/PhEXFBRNwfEfsi4pqK9W+MiKcj4o7y7/1195V0ZLQ7yUTAhFNtaIElaxCZ2Y2I36PodyAznweer/PBETEJXAu8BTgA7I6ImzLz3gWb/kVmvu0w95X0ArU6XfsfVKlOE9MtEfGLwOcyM1fw2ecA+zJzP0BE7AQuAep8yb+QfaXavrT3Eb5wx8FhhzFU9z3yjAlCleokiH8LHAe0I+JHFENdMzNftMx+p1Lcdd1zAHhtxXbnRsSdwEHgfZl5zwr2JSK2AdsANm/evPzRSH2u/+ZD7H7wSTa/+NhhhzI0G6YmuOgf/eSww9AaVOdO6sN9tGhVg+bCGsjtwOmZ+WxEXAR8Hthac99efDuAHQAzMzMrqeFItDpdXrXpRP74X5877FCkNWfZBBER51UtX/gAoQoHOPR+iU0UtYT+z3im7/WuiPj9iNhYZ1/pSGh1kqOnbV6RqtRpYvrNvtdHU/QP3Aa8eZn9dgNbI+IM4HvAZcC/6N8gIn4SeDQzMyLOoRhV9X3gB8vtKx0J7U6XqaNqzTgjjZ06TUwX97+PiNOAD9XYrx0RVwM3A5PAdZl5T0RcWa7fDvxz4N0R0Qb+Hris7Aiv3HdlhyYtr9VJO2ilAQ7np9MB4JV1NszMXcCuBcu2973+KPDRuvtKR1oxxNPx/1KVOn0Q/535DuIJ4NXAnQ3GJK2adjeZsgYhVapTg9jT97oNfDYz/7KheKRV1ep0mfYOYqlSnQTxp8CPMrMDxV3OEXFsZj7XbGhS89r2QUgD1bkybgWO6Xt/DPB/mglHWl2tTtdZTKUB6iSIozPz2d6b8vX43naqkeI8RNJgda6MH0bE2b03EfGzFENSpXWv3U2m7IOQKtXpg/gN4E8ioncn8ykUjyCV1r12J5mesgYhValzo9zuiHgF8NMUcyR9JzNbjUcmNSwzmXUUkzTQsj+dIuIq4LjM3JuZdwPHR8SvNx+a1KxOt/eoTWsQUpU6V8avZeYPem8y8yng1xqLSFol7TJB2EktVatzZUxExFwdvHza24bmQpJWx2zvWcwOc5Uq1emkvhn444jYTjHlxpXAFxuNSloF7U7ZxGQfhFSpToL4dxRPbHs3RSf1tylGMknrWrusQdgHIVVb9srIzC7wDWA/MAP8AnBfw3FJjWuVfRAbTBBSpYE1iIh4OcWDei6neIjPHwFk5ptWJzSpWa12rwZhE5NUZakmpu8AfwFcnJn7ACLivasSlbQK2l2bmKSlLHVl/CLwt8CfRcTHI+IXKPogpJHQ6vSamCzWUpWBCSIzb8zMtwOvAP4ceC9wckR8LCLOX6X4pMa0ep3UE9YgpCp1Oql/mJnXZ+bbgE3AHcA1TQcmNa1Xg7APQqq2op9OmflkZv5BZr65qYCk1dKeu1HOGoRUxStDY8upNqSleWVobM12HOYqLcUEobHVm2pj2k5qqZJXhsZW2xqEtCQThMbWrJ3U0pK8MjS25pqYrEFIlUwQGltOtSEtzStDY6tlDUJakglCY6s31YajmKRqXhkaW22n2pCWZILQ2Gp1HcUkLcUrQ2NrfhSTl4FUxStDY6vV6RIBkxM2MUlVTBAaW61O2kEtLcGrQ2Or3ek6xFVagglCY6vV6XqTnLQErw6NrVY3rUFISzBBaGy1O12fRy0twatDY6vdSaanrEFIg5ggNLZmO11HMUlLaPTqiIgLIuL+iNgXEddUrP/liLir/PtaRJzVt+7BiLg7Iu6IiD1Nxqnx1O6k02xIS5hq6oMjYhK4FngLcADYHRE3Zea9fZv9NfCGzHwqIi4EdgCv7Vv/psx8oqkYNd7a3a53UUtLaCxBAOcA+zJzP0BE7AQuAeYSRGZ+rW/7bwCbGoxnoG/u/z4nv+hotmw8bhj/vVbRE88+z3cffRaAx5+ddZirtIQmE8SpwMN97w9waO1goV8Bvtj3PoFbIiKBP8jMHVU7RcQ2YBvA5s2bDyvQd3ziW1xx7hb+/UU/c1j7a/14z2e+zdf3f3/u/XkvP2mI0UhrW5MJoqpxNys3jHgTRYL4+b7Fr8/MgxHxE8CXI+I7mfnVRR9YJI4dADMzM5Wfv5zpiYm5ZwNotD399y1+9vQf533n/zQALz/5+CFHJK1dTSaIA8Bpfe83AQcXbhQRrwL+ELgwM+d+2mXmwfLfxyLiRoomq0UJ4kiYnpqYm9lTo63V6XL6Ccdy7ktfMuxQpDWvyQbY3cDWiDgjIjYAlwE39W8QEZuBzwH/KjP/qm/5cRFxQu81cD6wt6lApybCGsSYaHfTfgeppsZqEJnZjoirgZuBSeC6zLwnIq4s128H3g+8BPj9iABoZ+YMcDJwY7lsCvhMZn6pqVinJyfmnk+s0dbqdJl2em+pliabmMjMXcCuBcu2973+VeBXK/bbD5y1cHlTpiaDdtcaxDhod9KhrVJNXin0ahAmiHFQzOBqDUKqwwRBrw/CJqZx0Op4c5xUl1cKRQ2ibQ1iLLS7yZR9EFItJghgejJod61BjINiBleLvVSHVwowNTnBbNsaxKjLzHIGV2sQUh0mCKxBjItOeY69D0KqxysFmJqwD2Ic9H4E2Ekt1eOVQvGFMesoppE3W/4I8DnUUj0mCMomJmsQI68335ajmKR6TBAUbdL2QYy+3o8A+yCkerxSKGoQ3kk9+lrlj4ANJgipFq8UfB7EuGi1ezUIm5ikOkwQlJP12Uk98noTMtrEJNXjlYKT9Y2L3nxb3ign1WOCwBvlxkVrbpirxV6qwyuFosnBGsTo69Ug7IOQ6jFBUDQ5tDpJprWIUda2BiGtiFcK818YHZuZRppTbUgr45XC/KgWHxo02mY7DnOVVsIEwfzcPC2fSz3S2nOjmCz2Uh1eKczPzeO9EKOtbQ1CWhETBMw9YcwJ+0Zbyz4IaUW8Uphvcpg1QYy03lQbTvct1WOCYL7JwSam0eZUG9LKeKUw/4XRtpN6pM1NtWENQqrFBAFsKL8wZtvWIEbZ3FQbjmKSavFKoXgmNViDGHVtp9qQVsQEwfwXhjfKjbbefS6OYpLq8Uph/gljDnMdbXM3ypkgpFq8UnCqjXHR6nSJgEmfByHVYoKgr4nJPoiR1uqkHdTSCni1MD+qxfsgRlu703WIq7QCJghgeqrXSW0NYpS1Ol1vkpNWwKuF+WGuJojR1uqmNQhpBUwQzN9ZaxPTaGt3unM/BiQtz6uF+WGP3ig32tqdnGtOlLQ8EwTzo5hmrUGMtNlO11FM0gp4tdA/iskaxChrd9JpNqQVMEHgdN/jot21D0JaCa8W5vsgvFFutLU6Off0QEnL82qhL0E43fdIa3W6TDvNhlRbowkiIi6IiPsjYl9EXFOxPiLiI+X6uyLi7Lr7HkmTE0GEo5hGnX0Q0so0liAiYhK4FrgQOBO4PCLOXLDZhcDW8m8b8LEV7HtETU9OOFnfiGt1u87kKq3AVIOffQ6wLzP3A0TETuAS4N6+bS4BPp2ZCXwjIk6MiFOALTX2PaKmJ4Kdux/i1vsebeq/0JA99ORzvP5lG4cdhrRuNJkgTgUe7nt/AHhtjW1OrbkvABGxjaL2webNmw872Kve/DL2fu/pw95fa9/Wk4/n0tdsGnYY0rrRZIKoauxd2IYzaJs6+xYLM3cAOwBmZmYOu43o19/4ssPdVZJGUpMJ4gBwWt/7TcDBmttsqLGvJKlBTfbY7Qa2RsQZEbEBuAy4acE2NwFXlKOZXgc8nZmP1NxXktSgxmoQmdmOiKuBm4FJ4LrMvCcirizXbwd2ARcB+4DngHcutW9TsUqSFotiANFomJmZyT179gw7DElaNyLitsycqVrnoHBJUiUThCSpkglCklTJBCFJqjRSndQR8TjwN4e5+0bgiSMYzjB5LGvPqBwHeCxr1eEey+mZeVLVipFKEC9EROwZ1JO/3ngsa8+oHAd4LGtVE8diE5MkqZIJQpJUyQQxb8ewAziCPJa1Z1SOAzyWteqIH4t9EJKkStYgJEmVTBCSpEpjnyAi4oKIuD8i9kXENcOOZ6Ui4sGIuDsi7oiIPeWyF0fElyPiu+W/Pz7sOKtExHUR8VhE7O1bNjD2iPit8jzdHxFvHU7U1QYcywci4nvlubkjIi7qW7eWj+W0iPiziLgvIu6JiH9TLl9X52aJ41h35yUijo6Ib0XEneWxfLBc3uw5ycyx/aOYSvwB4KcoHlJ0J3DmsONa4TE8CGxcsOxDwDXl62uA3x12nANiPw84G9i7XOzAmeX5OQo4ozxvk8M+hmWO5QPA+yq2XevHcgpwdvn6BOCvypjX1blZ4jjW3XmheMrm8eXraeCbwOuaPifjXoM4B9iXmfszcxbYCVwy5JiOhEuAT5WvPwX8s+GFMlhmfhV4csHiQbFfAuzMzOcz868pniFyzmrEWceAYxlkrR/LI5l5e/n674D7KJ4Tv67OzRLHMciaPA6ALDxbvp0u/5KGz8m4J4hTgYf73h9g6QK0FiVwS0TcFhHbymUnZ/FkPsp/f2Jo0a3coNjX67m6OiLuKpugetX/dXMsEbEFeA3FL9Z1e24WHAesw/MSEZMRcQfwGPDlzGz8nIx7goiKZett3O/rM/Ns4ELgqog4b9gBNWQ9nquPAS8FXg08AvxeuXxdHEtEHA/cAPxGZj6z1KYVy9bM8VQcx7o8L5nZycxXA5uAcyLilUtsfkSOZdwTxAHgtL73m4CDQ4rlsGTmwfLfx4AbKaqRj0bEKQDlv48NL8IVGxT7ujtXmfloeVF3gY8zX8Vf88cSEdMUX6rXZ+bnysXr7txUHcd6Pi8AmfkD4M+BC2j4nIx7gtgNbI2IMyJiA3AZcNOQY6otIo6LiBN6r4Hzgb0Ux/COcrN3AF8YToSHZVDsNwGXRcRREXEGsBX41hDiq6134ZYupTg3sMaPJSIC+B/AfZn5X/pWratzM+g41uN5iYiTIuLE8vUxwD8GvkPT52TYvfPD/gMuohjd8ADw28OOZ4Wx/xTFSIU7gXt68QMvAW4Fvlv+++Jhxzog/s9SVPFbFL94fmWp2IHfLs/T/cCFw46/xrH8T+Bu4K7ygj1lnRzLz1M0R9wF3FH+XbTezs0Sx7HuzgvwKuDbZcx7gfeXyxs9J061IUmqNO5NTJKkAUwQkqRKJghJUiUThCSpkglCklTJBCHVEBETEXFzRGwedizSanGYq1RDRLwU2JSZXxl2LNJqMUFIy4iIDsWNVT07M/M/DyseabWYIKRlRMSzmXn8sOOQVpt9ENJhiuJpfr9bPunrWxHxsnL56RFxazmd9K29fouIODkibiyfCnZnRPxcufzz5XTt9/SmbC+ndv5kROyN4omB7x3ekWpcTQ07AGkdOKach7/nP2XmH5Wvn8nMcyLiCuC/AW8DPgp8OjM/FRHvAj5C8SCXjwBfycxLI2IS6NVK3pWZT5aTsO2OiBuALcCpmflKgN5EbdJqsolJWsagJqaIeBB4c2buL6eV/tvMfElEPEExAVyrXP5IZm6MiMcpOrqfX/A5H6CYVRSKxPBWignW9gC7gP8N3JLF9NTSqrGJSXphcsDrQdscIiLeSDF187mZeRbFjJ1HZ+ZTwFkU8/5fBfzhEYhVWhEThPTCvL3v36+Xr79G8WwRgF8G/l/5+lbg3TDXx/Ai4MeApzLzuYh4BcWD6ImIjcBEZt4A/Afg7KYPRFrIJiZpGRXDXL+UmdeUTUyfoHjGwARweWbuK59/fB2wEXgceGdmPhQRJwM7KJ7j0aFIFrcDn6d4XvD9wEnAB4Cnys/u/Yj7rcz8YnNHKS1mgpAOU5kgZjLziWHHIjXBJiZJUiVrEJKkStYgJEmVTBCSpEomCElSJROEJKmSCUKSVOn/A9UvTUbKJ0zIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "e = range(epocas)\n",
    "plt.plot(e,acc_tr)\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.yticks(np.arange(0, 1.25, step=0.25))\n",
    "#plt.savefig('ej6sit2_accepoch0p5.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(e,loss_tr)\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss')\n",
    "#plt.savefig('ej6sit2_lossepoch0p5.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
