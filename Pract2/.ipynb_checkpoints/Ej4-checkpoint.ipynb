{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "#using tensorfow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok funciona bien\n",
    "#funcion que hace reshape de las imagenes\n",
    "#sin agregar el bias\n",
    "def reshapeImages(x,y):\n",
    "    im_shape = x.shape[1:]\n",
    "    x = np.reshape(x,(x.shape[0],np.prod(im_shape)))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok funciona bien\n",
    "#crea una matriz random pasandosele las dimensiones\n",
    "#hay que tener en cuenta el bias afuera\n",
    "#solo recibe las dimensiones y devuelve matriz de d1 x d2\n",
    "def createRandomMatrix(d1,d2):\n",
    "    W = np.random.rand(d1,d2) * 1e-4 #coeficientes en un rango [0,1e-3)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok funciona bien\n",
    "#randomiza filas de una matriz\n",
    "#es para shufflear las imagenes antes de tomar los batchs\n",
    "def randomizeMatrixRows(x,y):\n",
    "    #x matriz de imagenes (imagenes x dimension)\n",
    "    #y matriz de scores verdaderos (imagenes x categorias)\n",
    "    indices = np.random.choice(x.shape[0], x.shape[0], replace=False)\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok funciona bien\n",
    "#adds una columna de 1's al comienzo de x\n",
    "#donde x va a ser la matriz de las imagenes (imagenes x 3072)\n",
    "#retorna (imagenes x 3073) donde la primera columna es de 1's\n",
    "def addBias(x):\n",
    "    aux = np.ones((x.shape[0],1))\n",
    "    x = np.hstack((aux,x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy metrica\n",
    "#le paso los scores de todas las imagenes (imagenes x categorias)\n",
    "#y la matriz y formato tipo (imagenes x categorias)\n",
    "def accuracy(scores,y):\n",
    "    y_pred = np.argmax(scores,axis=1) #selecciona categoria correcta\n",
    "    y_true = np.argmax(y,axis=1) #selecciona categoria correcta\n",
    "    acc = np.mean(y_pred == y_true) #calculo accuracy\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrica MSE // mean square error\n",
    "#le paso los scores de todas las imagenes (imagenes x categorias)\n",
    "#y la matriz y formato tipo (imagenes x categorias)\n",
    "def MSE(scores,y):\n",
    "    mse = np.mean(np.sum((scores-y)**2,axis=1))\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradiente metrica MSE\n",
    "def grad_MSE(scores,y):\n",
    "    gradmse = (scores-y)*2\n",
    "    return gradmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function para softmax\n",
    "#le paso los scores de todas las imagenes (imagenes x categorias)\n",
    "#y la matriz y formato tipo (imagenes x categorias)\n",
    "#idem argumentos que MSE\n",
    "def loss_softmax(scores,y):\n",
    "    #estabilidad numerica como haciamos en softmax \n",
    "    #basicamente lo de la practica anterior, cambio un poco los nombres para que me quede mas claro\n",
    "    scoresmax = scores.max(axis=1) #maximo de cada fila\n",
    "    scores = scores - scoresmax[:,np.newaxis] #le resto a cada fila el maximo correspondiente\n",
    "    y = np.argmax(y,axis=1)\n",
    "    scores_yi = scores[np.arange(scores.shape[0]),y] #estos son los f_yi\n",
    "    expscores = np.exp(scores) #hace exp(scores)\n",
    "    sum_expscores = expscores.sum(axis=1) #suma de los exp(scores) por fila \n",
    "    loss = np.log(sum_expscores) - scores_yi #aca estan las loss_i en vector fila\n",
    "    loss = loss.mean() #vector fila, solo hago el mean, no sobre algun axis particular\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradiente para softmax\n",
    "#idem practica anterior, cambio los nombres para que sea mas claro\n",
    "def grad_softmax(scores,y):\n",
    "    y = np.argmax(y,axis=1)\n",
    "    y = y.flatten()\n",
    "    scoresmax = scores.max(axis=1) #maximo de cada fila\n",
    "    scores = scores - scoresmax[:,np.newaxis] #le resto a cada fila el maximo correspondiente\n",
    "    scores_yi = scores[np.arange(scores.shape[0]),y] #estos son los f_yi\n",
    "    expscores = np.exp(scores) #hace exp(scores)\n",
    "    sum_expscores = expscores.sum(axis=1) #suma de los exp(scores) por fila \n",
    "    grad = (1/sum_expscores)[:,np.newaxis]*expscores\n",
    "    grad[np.arange(y.shape[0]),y] = grad[np.arange(y.shape[0]),y] - 1\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion sigmoide\n",
    "def sigmoid(x):\n",
    "    s = (1+np.exp(-x))**(-1)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivada de la funcion sigmoide\n",
    "def grad_sigmoid(x):\n",
    "    gs = (np.exp(-x))/((1+np.exp(-x))**2)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#le paso los datos de training x_train, y_train formato (imagenesxdimension, imagenesxcategorias)\n",
    "#le paso los datos de test x_test,y_test idem formato\n",
    "#bs es el batch size por defecto es = 100\n",
    "#epocas es la cantidad de epocas por defecto es = 100\n",
    "#lr learning rate (alpha practica anterior) por defecto 1e-3\n",
    "#rg la cte para la regularizacion (lam/lambda practica anterior) por defecto 1e-5\n",
    "#n_capa1 cantidad de neuronas capa 1\n",
    "#n_capa2 cantidad de neuronas capa 2 (=n_clases)\n",
    "\n",
    "def fit(x_train,y_train,x_test,y_test,n_capa1,n_capa2,bs=100,epocas=100,lr=1e-3,rg=1e-5):\n",
    "    #renombro algunas constantes utiles\n",
    "    dim = x_train.shape[1] #dimension del problema (cifar10 = 3072)\n",
    "    nimg = x_train.shape[0] #numero de ejemplos del training dataset\n",
    "    #creo las matrices de pesos w1 y w2 para capa 1 y 2 respectivamente\n",
    "    w1 = createRandomMatrix(dim+1,n_capa1) # w1 de 3073 x n_capa1 para cifar10 \n",
    "    w2 = createRandomMatrix(n_capa1+1,n_capa2) # w2 de n_capa1 + 1 x n_capa2 (que es n_capa1 10 para cifar 10) \n",
    "    it = int((x_train.shape[0])/(bs)) #cantidad de iteraciones por epoca, convierto a int para iterar\n",
    "    #variables a devolver para graficar, ordenadas por epoca\n",
    "    test_acc = []\n",
    "    test_loss = []\n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    #empiezo el loop en las epocas\n",
    "    for i in range(epocas):\n",
    "        print(\"epoca: \",i)\n",
    "        loss = 0 #loss o funcion de costo a acumular en iteraciones\n",
    "        acc = 0 #accuracy a acumular en iteraciones\n",
    "        x_train,y_train = randomizeMatrixRows(x_train,y_train) #shuffle de los datos\n",
    "        #loop en las iteraciones\n",
    "        for j in range(it):\n",
    "            x_batch = x_train[j*bs:(j+1)*bs,:] #seleccion de batch\n",
    "            y_batch = y_train[j*bs:(j+1)*bs] #seleccion de batch\n",
    "            x_batch = addBias(x_batch) #aniadimos el bias\n",
    "            #Forward path\n",
    "            s1 = sigmoid(x_batch.dot(w1)) #salida s1 primera capa\n",
    "            s1 = addBias(s1) #aniado el bias\n",
    "            s2 = s1.dot(w2)  #salida s2 segunda capa\n",
    "            #regularizacion\n",
    "            reg1 = np.sum(w1**2) #regularizacion matriz w1\n",
    "            reg2 = np.sum(w2**2) #regularizacion matriz w2\n",
    "            reg = rg*(reg1 + reg2) #tomo igual constante de reg para los dos\n",
    "            #calculo loss\n",
    "            loss = loss + loss_softmax(s2,y_batch) + 0.5*reg #con la convencion del * 0.5\n",
    "            #calculo accuracy\n",
    "            acc = acc + accuracy(s2,y_batch)\n",
    "            #Backwards path\n",
    "            grad = grad_softmax(s2,y_batch) #primer paso grad es el gradiente global\n",
    "            #Capa 2\n",
    "            gradw2 = (s1.T).dot(grad) #capa 2 gradiente respecto w2\n",
    "            grad = grad.dot(w2.T) #multiplicamos grad global * grad local\n",
    "            grad = grad[:,1:] #todas las filas sacamos la columna del bias\n",
    "            #Capa 1\n",
    "            grad = grad * grad_sigmoid(x_batch.dot(w1)) #grad global * grad_sigmoid(y1) local\n",
    "            gradw1 = (x_batch.T).dot(grad) #grad respecto w1\n",
    "            #paso de optimizacion, actualizamos w1 y w2\n",
    "            w1 -= lr*(gradw1+rg*w1)\n",
    "            w2 -= lr*(gradw2+rg*w2)\n",
    "        #fin de analisis de todos los batchs, seguimos y despues volvemos a entrenar por batchs\n",
    "        #promediamos loss y accuracy sumados por batchs\n",
    "        loss = loss/(it)\n",
    "        acc = acc/(it)\n",
    "        print(\"train data: \",loss,acc)\n",
    "        train_acc.append(acc)\n",
    "        train_loss.append(loss)\n",
    "        #calculamos accuracy con los datos de test\n",
    "        #forward con x_test basicamente\n",
    "        x_ts = addBias(x_test)\n",
    "        s1 = sigmoid(x_ts.dot(w1)) #salida s1 primera capa\n",
    "        s1 = addBias(s1) #aniado el bias\n",
    "        s2 = s1.dot(w2)  #salida s2 segunda capa\n",
    "        #regularizacion\n",
    "        reg1 = np.sum(w1**2) #regularizacion matriz w1\n",
    "        reg2 = np.sum(w2**2) #regularizacion matriz w2\n",
    "        reg = rg*(reg1 + reg2) #tomo igual constante de reg para los dos\n",
    "        #calculo loss\n",
    "        loss = loss_softmax(s2,y_test) + 0.5*reg #con la convencion del * 0.5\n",
    "        #calculo accuracy\n",
    "        acc = accuracy(s2,y_test)\n",
    "        test_acc.append(acc)\n",
    "        test_loss.append(loss)\n",
    "        print(\"train data: \",loss,acc)\n",
    "    return train_acc,train_loss,test_acc,test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#definicion de constantes\n",
    "n_clases = 10 #cantidad de clases\n",
    "n_capa1 = 100 #cantidad de neuronas capa 1\n",
    "n_capa2 = n_clases #cantidad de neuronas capa 2 (= cantidad de clases)\n",
    "#importamos los datos de training y testing\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data() #x = images #y = categories #loading data\n",
    "#reshaping y flatten\n",
    "x_train,y_train = reshapeImages(x_train,y_train)\n",
    "x_test,y_test = reshapeImages(x_test,y_test)\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "#pasamos al formato del ejercicio, creo los arrays\n",
    "yy_train = np.zeros([x_train.shape[0],n_clases])\n",
    "yy_test = np.zeros([x_test.shape[0],n_clases])\n",
    "#pongo un 1 en la categoria correcta\n",
    "yy_train[np.arange(x_train.shape[0]),y_train] = 1\n",
    "yy_test[np.arange(x_test.shape[0]),y_test] = 1\n",
    "#restamos una mean image a todas las imagenes (chequeo broadcasting ok)\n",
    "x_train = x_train - np.mean(x_train,axis=0)\n",
    "x_test = x_test - np.mean(x_train,axis=0)\n",
    "st = time.time()\n",
    "epocas = 200\n",
    "#def fit(x_train,y_train,x_test,y_test,n_capa1,n_capa2,bs=100,epocas=100,lr=1e-3,rg=1e-5):\n",
    "#train_acc,train_loss,test_acc,test_loss = fit(x_train,yy_train,x_test,yy_test,100,10,100,epocas,1e-5,1e-2)\n",
    "et = time.time()\n",
    "print(et-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-7c1151bc14c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#train_acc y test_acc en funcion de epocas\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"test accuracy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Épocas\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_acc' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ahora tengo los datos train_acc,train_loss,test_acc,test_loss\n",
    "epocas = 200\n",
    "e = range(epocas)\n",
    "plt.figure(1)\n",
    "#train_acc y test_acc en funcion de epocas\n",
    "plt.plot(e,train_acc,label=\"train accuracy\")\n",
    "plt.plot(e,test_acc,label=\"test accuracy\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"epochaccex43.pdf\")\n",
    "#train_loss y test_loss en funcion de epocas\n",
    "plt.figure(2)\n",
    "plt.plot(e,train_loss,label=\"train loss\")\n",
    "plt.plot(e,test_loss,label=\"test loss\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"epochlossex43.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
