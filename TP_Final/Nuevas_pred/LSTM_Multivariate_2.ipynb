{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler #scaling de los datos entre 0 y 1\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "plt.style.use('seaborn')\n",
    "plt.style.use('matplotlibrc.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "  dic_localidades = {\n",
    "        'RiesgoBariloche':'Bariloche',\n",
    "        'RiesgoBuenosAires':'Buenos Aires',\n",
    "        'RiesgoCABACABANA':'CABA',\n",
    "        'RiesgoChacoNA':'Chaco',\n",
    "        'RiesgoCórdobaCórdoba':'Córdoba',\n",
    "        'RiesgoEntreRiosRíos':'Entre Ríos',\n",
    "        'RiesgoJujuyJujuy':'Jujuy',\n",
    "        'RiesgoLaRiojaRioja':'La Rioja',\n",
    "        'RiesgoMendozaMendoza':'Mendoza',\n",
    "        'RiesgoNeuquénNeuquén':'Neuquén',\n",
    "        'RiesgoRioNegro':'Río Negro',\n",
    "        'RiesgoSaltaSalta':'Salta',\n",
    "        'RiesgoSantaCruzSantaCruz':'Santa Cruz',\n",
    "        'RiesgoSantaFeSantaFe':'Santa Fe',\n",
    "        'RiesgoTierradelFuegoTierradel':'Tierra del Fuego',\n",
    "        'RiesgoTucumanTucuman':'Tucumán'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(sequence, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\tif out_end_ix > len(sequence):\n",
    "\t\t\tbreak\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_predictions(l,df_original,y_train_pr,y_test_pr,name):\n",
    "    plt.plot(df_original) \n",
    "    months_tr = np.arange(l,len(y_train_pr)+l) #meses de training\n",
    "    months_ts = np.arange(len(y_train_pr)+(2*l)+1,len(df_original)-1) #meses de testing\n",
    "    plt.plot(months_tr,y_train_pr,label='train') #grafico de train results\n",
    "    plt.plot(months_ts,y_test_pr,label='test') #grafico de test results\n",
    "    plt.title(dic_localidades[str(name)])\n",
    "    plt.xlabel('Días')\n",
    "    plt.ylabel('Casos/100 mil hab')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.savefig(dic_localidades[str(name)]+'_fit_2.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'Datos'\n",
    "files = [f for f in listdir(mypath) if isfile(join(mypath, f))] #get all file names of that path\n",
    "df_train_total = [] #aca guardamos todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "n_steps_in = 7\n",
    "n_steps_out = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# armo scaler para los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RiesgoBariloche\n",
      "RiesgoBuenosAires\n",
      "RiesgoCABACABANA\n",
      "RiesgoChacoNA\n",
      "RiesgoCórdobaCórdoba\n",
      "RiesgoEntreRiosRíos\n",
      "RiesgoJujuyJujuy\n",
      "RiesgoLaRiojaRioja\n",
      "RiesgoMendozaMendoza\n",
      "RiesgoNeuquénNeuquén\n",
      "RiesgoRioNegro\n",
      "RiesgoSaltaSalta\n",
      "RiesgoSantaCruzSantaCruz\n",
      "RiesgoSantaFeSantaFe\n",
      "RiesgoTierradelFuegoTierradel\n",
      "RiesgoTucumanTucuman\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print(file)\n",
    "    data = pd.read_csv(mypath+str('/')+file,sep=\",\",quotechar='\"',na_values=[''])\n",
    "    data = data[\"incidenciaAcum14d\"]\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.to_numpy()\n",
    "    df = df[:-20]\n",
    "    total_size = df.shape[0]\n",
    "    train_size = int(0.8*total_size)\n",
    "    test_size = total_size - train_size\n",
    "    train_data = df[:-test_size]\n",
    "    df_train_total.extend(list(train_data.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_train_total = scaler.fit_transform(np.array(df_train_total).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ahora armo los datos de train, test y forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast = [] #aca van a estar los datos para hacer el forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RiesgoBariloche\n",
      "RiesgoBuenosAires\n",
      "RiesgoCABACABANA\n",
      "RiesgoChacoNA\n",
      "RiesgoCórdobaCórdoba\n",
      "RiesgoEntreRiosRíos\n",
      "RiesgoJujuyJujuy\n",
      "RiesgoLaRiojaRioja\n",
      "RiesgoMendozaMendoza\n",
      "RiesgoNeuquénNeuquén\n",
      "RiesgoRioNegro\n",
      "RiesgoSaltaSalta\n",
      "RiesgoSantaCruzSantaCruz\n",
      "RiesgoSantaFeSantaFe\n",
      "RiesgoTierradelFuegoTierradel\n",
      "RiesgoTucumanTucuman\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "    print(file)\n",
    "    data = pd.read_csv(mypath+str('/')+file,sep=\",\",quotechar='\"',na_values=[''])\n",
    "    data = data[\"incidenciaAcum14d\"]\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.to_numpy()\n",
    "    df_forecast.append(df[-20:-10])\n",
    "    df = df[:-20]\n",
    "    total_size = df.shape[0]\n",
    "    train_size = int(0.8*total_size)\n",
    "    test_size = total_size - train_size\n",
    "    train_data = df[:-test_size]\n",
    "    test_data = df[-test_size:]\n",
    "    train_data = scaler.transform(train_data.reshape(-1,1))\n",
    "    test_data = scaler.transform(test_data.reshape(-1,1))\n",
    "    if file == 'RiesgoBariloche':\n",
    "        x_train_total, y_train_total = split_sequence(train_data, n_steps_in, n_steps_out)\n",
    "        x_test_total, y_test_total = split_sequence(test_data, n_steps_in, n_steps_out)\n",
    "    else:\n",
    "        #train\n",
    "        x_train, y_train = split_sequence(train_data, n_steps_in, n_steps_out)\n",
    "        x_train_total = np.vstack((x_train_total,x_train))\n",
    "        y_train_total = np.vstack((y_train_total,y_train))\n",
    "        #test\n",
    "        x_test, y_test = split_sequence(test_data, n_steps_in, n_steps_out)\n",
    "        x_test_total = np.vstack((x_test_total,x_test))\n",
    "        y_test_total = np.vstack((y_test_total,y_test))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_total = x_train_total.reshape(x_train_total.shape[0],1,x_train_total.shape[1])\n",
    "x_test_total = x_test_total.reshape(x_test_total.shape[0],1,x_test_total.shape[1])\n",
    "y_train_total =  y_train_total.reshape(y_train_total.shape[0],y_train_total.shape[1])\n",
    "y_test_total = y_test_total.reshape(y_test_total.shape[0],y_test_total.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 128)            69632     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 202,119\n",
      "Trainable params: 202,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3334 samples, validate on 686 samples\n",
      "Epoch 1/400\n",
      " - 1s - loss: 0.0411 - mse: 0.0411 - val_loss: 0.6628 - val_mse: 0.6628\n",
      "Epoch 2/400\n",
      " - 0s - loss: 0.0162 - mse: 0.0162 - val_loss: 3.6861 - val_mse: 3.6861\n",
      "Epoch 3/400\n",
      " - 0s - loss: 0.0044 - mse: 0.0044 - val_loss: 3.0585 - val_mse: 3.0585\n",
      "Epoch 4/400\n",
      " - 0s - loss: 0.0029 - mse: 0.0029 - val_loss: 1.2310 - val_mse: 1.2310\n",
      "Epoch 5/400\n",
      " - 0s - loss: 0.0022 - mse: 0.0022 - val_loss: 0.3528 - val_mse: 0.3528\n",
      "Epoch 6/400\n",
      " - 0s - loss: 0.0018 - mse: 0.0018 - val_loss: 0.1194 - val_mse: 0.1194\n",
      "Epoch 7/400\n",
      " - 0s - loss: 0.0015 - mse: 0.0015 - val_loss: 0.0448 - val_mse: 0.0448\n",
      "Epoch 8/400\n",
      " - 0s - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0372 - val_mse: 0.0372\n",
      "Epoch 9/400\n",
      " - 0s - loss: 0.0013 - mse: 0.0013 - val_loss: 0.0385 - val_mse: 0.0385\n",
      "Epoch 10/400\n",
      " - 0s - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0458 - val_mse: 0.0458\n",
      "Epoch 11/400\n",
      " - 0s - loss: 0.0012 - mse: 0.0012 - val_loss: 0.0577 - val_mse: 0.0577\n",
      "Epoch 12/400\n",
      " - 0s - loss: 0.0011 - mse: 0.0011 - val_loss: 0.0449 - val_mse: 0.0449\n",
      "Epoch 13/400\n",
      " - 0s - loss: 0.0010 - mse: 0.0010 - val_loss: 0.0448 - val_mse: 0.0448\n",
      "Epoch 14/400\n",
      " - 0s - loss: 9.8569e-04 - mse: 9.8569e-04 - val_loss: 0.0496 - val_mse: 0.0496\n",
      "Epoch 15/400\n",
      " - 0s - loss: 8.7458e-04 - mse: 8.7458e-04 - val_loss: 0.0489 - val_mse: 0.0489\n",
      "Epoch 16/400\n",
      " - 0s - loss: 7.7172e-04 - mse: 7.7172e-04 - val_loss: 0.0431 - val_mse: 0.0431\n",
      "Epoch 17/400\n",
      " - 0s - loss: 6.7690e-04 - mse: 6.7690e-04 - val_loss: 0.0384 - val_mse: 0.0384\n",
      "Epoch 18/400\n",
      " - 0s - loss: 6.2171e-04 - mse: 6.2171e-04 - val_loss: 0.0404 - val_mse: 0.0404\n",
      "Epoch 19/400\n",
      " - 0s - loss: 6.0968e-04 - mse: 6.0968e-04 - val_loss: 0.0344 - val_mse: 0.0344\n",
      "Epoch 20/400\n",
      " - 0s - loss: 5.4769e-04 - mse: 5.4769e-04 - val_loss: 0.0469 - val_mse: 0.0469\n",
      "Epoch 21/400\n",
      " - 0s - loss: 5.4517e-04 - mse: 5.4517e-04 - val_loss: 0.0443 - val_mse: 0.0443\n",
      "Epoch 22/400\n",
      " - 0s - loss: 5.4516e-04 - mse: 5.4516e-04 - val_loss: 0.0369 - val_mse: 0.0369\n",
      "Epoch 23/400\n",
      " - 0s - loss: 5.5300e-04 - mse: 5.5300e-04 - val_loss: 0.0398 - val_mse: 0.0398\n",
      "Epoch 24/400\n",
      " - 0s - loss: 5.0072e-04 - mse: 5.0072e-04 - val_loss: 0.0468 - val_mse: 0.0468\n",
      "Epoch 25/400\n",
      " - 0s - loss: 4.7823e-04 - mse: 4.7823e-04 - val_loss: 0.0620 - val_mse: 0.0620\n",
      "Epoch 26/400\n",
      " - 0s - loss: 4.7784e-04 - mse: 4.7784e-04 - val_loss: 0.0606 - val_mse: 0.0606\n",
      "Epoch 27/400\n",
      " - 0s - loss: 4.9623e-04 - mse: 4.9623e-04 - val_loss: 0.0427 - val_mse: 0.0427\n",
      "Epoch 28/400\n",
      " - 0s - loss: 4.8923e-04 - mse: 4.8923e-04 - val_loss: 0.0557 - val_mse: 0.0557\n",
      "Epoch 29/400\n",
      " - 0s - loss: 4.5990e-04 - mse: 4.5990e-04 - val_loss: 0.0493 - val_mse: 0.0493\n",
      "Epoch 30/400\n",
      " - 0s - loss: 5.1000e-04 - mse: 5.1000e-04 - val_loss: 0.1010 - val_mse: 0.1010\n",
      "Epoch 31/400\n",
      " - 0s - loss: 7.2435e-04 - mse: 7.2435e-04 - val_loss: 0.0555 - val_mse: 0.0555\n",
      "Epoch 32/400\n",
      " - 0s - loss: 4.7936e-04 - mse: 4.7936e-04 - val_loss: 0.0481 - val_mse: 0.0481\n",
      "Epoch 33/400\n",
      " - 0s - loss: 4.5245e-04 - mse: 4.5245e-04 - val_loss: 0.0441 - val_mse: 0.0441\n",
      "Epoch 34/400\n",
      " - 0s - loss: 4.4834e-04 - mse: 4.4834e-04 - val_loss: 0.0410 - val_mse: 0.0410\n",
      "Epoch 35/400\n",
      " - 0s - loss: 4.5021e-04 - mse: 4.5021e-04 - val_loss: 0.0394 - val_mse: 0.0394\n",
      "Epoch 36/400\n",
      " - 0s - loss: 4.5304e-04 - mse: 4.5304e-04 - val_loss: 0.0485 - val_mse: 0.0485\n",
      "Epoch 37/400\n",
      " - 0s - loss: 4.4353e-04 - mse: 4.4353e-04 - val_loss: 0.0412 - val_mse: 0.0412\n",
      "Epoch 38/400\n",
      " - 0s - loss: 4.5976e-04 - mse: 4.5976e-04 - val_loss: 0.0487 - val_mse: 0.0487\n",
      "Epoch 39/400\n",
      " - 0s - loss: 4.4390e-04 - mse: 4.4390e-04 - val_loss: 0.0452 - val_mse: 0.0452\n",
      "Epoch 40/400\n",
      " - 0s - loss: 4.4227e-04 - mse: 4.4227e-04 - val_loss: 0.0539 - val_mse: 0.0539\n",
      "Epoch 41/400\n",
      " - 0s - loss: 4.4680e-04 - mse: 4.4680e-04 - val_loss: 0.0512 - val_mse: 0.0512\n",
      "Epoch 42/400\n",
      " - 0s - loss: 4.4877e-04 - mse: 4.4877e-04 - val_loss: 0.0400 - val_mse: 0.0400\n",
      "Epoch 43/400\n",
      " - 0s - loss: 4.8448e-04 - mse: 4.8448e-04 - val_loss: 0.0419 - val_mse: 0.0419\n",
      "Epoch 44/400\n",
      " - 0s - loss: 5.1918e-04 - mse: 5.1918e-04 - val_loss: 0.0509 - val_mse: 0.0509\n",
      "Epoch 45/400\n",
      " - 0s - loss: 4.4544e-04 - mse: 4.4544e-04 - val_loss: 0.0558 - val_mse: 0.0558\n",
      "Epoch 46/400\n",
      " - 0s - loss: 4.9145e-04 - mse: 4.9145e-04 - val_loss: 0.0557 - val_mse: 0.0557\n",
      "Epoch 47/400\n",
      " - 0s - loss: 4.2292e-04 - mse: 4.2292e-04 - val_loss: 0.0462 - val_mse: 0.0462\n",
      "Epoch 48/400\n",
      " - 0s - loss: 4.1881e-04 - mse: 4.1881e-04 - val_loss: 0.0497 - val_mse: 0.0497\n",
      "Epoch 49/400\n",
      " - 0s - loss: 4.1613e-04 - mse: 4.1613e-04 - val_loss: 0.0547 - val_mse: 0.0547\n",
      "Epoch 50/400\n",
      " - 0s - loss: 4.2235e-04 - mse: 4.2235e-04 - val_loss: 0.0555 - val_mse: 0.0555\n",
      "Epoch 51/400\n",
      " - 0s - loss: 4.6572e-04 - mse: 4.6572e-04 - val_loss: 0.0548 - val_mse: 0.0548\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(units=128,activation='relu',return_sequences=True,input_shape=(1,n_steps_in)))\n",
    "model.add(keras.layers.LSTM(units=128,activation='relu'))\n",
    "model.add(keras.layers.Dense(units=n_steps_out))\n",
    "model.compile(optimizer='adam',loss=keras.losses.MSE,metrics=['mse']) \n",
    "model.summary()\n",
    "history = model.fit(x_train_total, y_train_total,epochs=400,batch_size=128,validation_data=(x_test_total,y_test_total),verbose=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'],label='train')\n",
    "plt.plot(history.history['val_loss'],label='test')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pr = model.predict(x_train_total)\n",
    "y_test_pr = model.predict(x_test_total)\n",
    "y_train_pr = scaler.inverse_transform(y_train_pr)\n",
    "y_test_pr = scaler.inverse_transform(y_test_pr)\n",
    "y_train =  scaler.inverse_transform(y_train_total)\n",
    "y_test = scaler.inverse_transform(y_test_total)\n",
    "print('train mse squared:',mean_squared_error(y_train_total,y_train)) \n",
    "print('test mse squared:',mean_squared_error(y_test_total,y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# veo como se ajustan a los train y test data para distintas localidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = tf.keras.losses.MeanAbsolutePercentageError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    print(file)\n",
    "    data = pd.read_csv(mypath+str('/')+file,sep=\",\",quotechar='\"',na_values=[''])\n",
    "    data = data[\"incidenciaAcum14d\"]\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.to_numpy()\n",
    "    df = df[:-10]\n",
    "    y_forecast = np.copy(df[-n_steps_in:])\n",
    "    df = df[:-n_steps_in]\n",
    "    x_toforecast = np.copy(df[-n_steps_in:])\n",
    "    x_toforecast = scaler.transform(x_toforecast.reshape(-1,1))\n",
    "    x_toforecast = x_toforecast.flatten()\n",
    "    x_toforecast = x_toforecast.reshape(1,1,n_steps_in)\n",
    "    y_forecasted = model.predict(x_toforecast)\n",
    "    y_forecasted = scaler.inverse_transform(y_forecasted)\n",
    "    plt.title(dic_localidades[str(file)])\n",
    "    plt.plot(y_forecasted.flatten(),'o',label='forecasted')\n",
    "    plt.plot(y_forecast,label='true data')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
