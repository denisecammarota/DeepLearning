{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler #scaling de los datos entre 0 y 1\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import r2_score\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "plt.style.use('seaborn')\n",
    "plt.style.use('matplotlibrc.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  dic_localidades = {\n",
    "        'RiesgoBariloche':'Bariloche',\n",
    "        'RiesgoBuenosAires':'Buenos Aires',\n",
    "        'RiesgoCABACABANA':'CABA',\n",
    "        'RiesgoChacoNA':'Chaco',\n",
    "        'RiesgoCórdobaCórdoba':'Córdoba',\n",
    "        'RiesgoEntreRiosRíos':'Entre Ríos',\n",
    "        'RiesgoJujuyJujuy':'Jujuy',\n",
    "        'RiesgoLaRiojaRioja':'La Rioja',\n",
    "        'RiesgoMendozaMendoza':'Mendoza',\n",
    "        'RiesgoNeuquénNeuquén':'Neuquén',\n",
    "        'RiesgoRioNegro':'Río Negro',\n",
    "        'RiesgoSaltaSalta':'Salta',\n",
    "        'RiesgoSantaCruzSantaCruz':'Santa Cruz',\n",
    "        'RiesgoSantaFeSantaFe':'Santa Fe',\n",
    "        'RiesgoTierradelFuegoTierradel':'Tierra del Fuego',\n",
    "        'RiesgoTucumanTucuman':'Tucumán'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_predictions(l,df_original,y_train_pr,y_test_pr,name):\n",
    "    plt.plot(df_original) \n",
    "    months_tr = np.arange(l,len(y_train_pr)+l) #meses de training\n",
    "    months_ts = np.arange(len(y_train_pr)+(2*l)+1,len(df_original)-1) #meses de testing\n",
    "    plt.plot(months_tr,y_train_pr,label='train') #grafico de train results\n",
    "    plt.plot(months_ts,y_test_pr,label='test') #grafico de test results\n",
    "    plt.title(dic_localidades[str(name)])\n",
    "    plt.xlabel('Días')\n",
    "    plt.ylabel('Casos/100 mil hab')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.savefig(dic_localidades[str(name)]+'_fit_2.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'Datos'\n",
    "files = [f for f in listdir(mypath) if isfile(join(mypath, f))] #get all file names of that path\n",
    "df_train_total = [] #aca guardamos todos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "tw = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# armo scaler para los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    print(file)\n",
    "    data = pd.read_csv(mypath+str('/')+file,sep=\",\",quotechar='\"',na_values=[''])\n",
    "    data = data[\"incidenciaAcum14d\"]\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.to_numpy()\n",
    "    df = df[:-20]\n",
    "    total_size = df.shape[0]\n",
    "    train_size = int(0.8*total_size)\n",
    "    test_size = total_size - train_size\n",
    "    train_data = df[:-test_size]\n",
    "    df_train_total.extend(list(train_data.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_train_total = scaler.fit_transform(np.array(df_train_total).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ahora armo los datos de train, test y forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast = [] #aca van a estar los datos para hacer el forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    print(file)\n",
    "    data = pd.read_csv(mypath+str('/')+file,sep=\",\",quotechar='\"',na_values=[''])\n",
    "    data = data[\"incidenciaAcum14d\"]\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.to_numpy()\n",
    "    df_forecast.append(df[-20:-10])\n",
    "    df = df[:-20]\n",
    "    total_size = df.shape[0]\n",
    "    train_size = int(0.8*total_size)\n",
    "    test_size = total_size - train_size\n",
    "    train_data = df[:-test_size]\n",
    "    test_data = df[-test_size:]\n",
    "    train_data = scaler.transform(train_data.reshape(-1,1))\n",
    "    test_data = scaler.transform(test_data.reshape(-1,1))\n",
    "    if file == 'RiesgoBariloche':\n",
    "        x_train_total, y_train_total = create_dataset(train_data,tw)\n",
    "        x_test_total, y_test_total = create_dataset(test_data,tw)\n",
    "    else:\n",
    "        #train\n",
    "        x_train, y_train = create_dataset(train_data,tw)\n",
    "        x_train_total = np.vstack((x_train_total,x_train))\n",
    "        y_train_total = np.hstack((y_train_total,y_train))\n",
    "        #test\n",
    "        x_test, y_test = create_dataset(test_data,tw)\n",
    "        x_test_total = np.vstack((x_test_total,x_test))\n",
    "        y_test_total = np.hstack((y_test_total,y_test))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_total = x_train_total.reshape(x_train_total.shape[0],1,x_train_total.shape[1])\n",
    "x_test_total = x_test_total.reshape(x_test_total.shape[0],1,x_test_total.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(units=128,activation='relu',return_sequences=True,input_shape=(1,tw)))\n",
    "model.add(keras.layers.LSTM(units=128,activation='relu'))\n",
    "model.add(keras.layers.Dense(units=1))\n",
    "model.compile(optimizer='adam',loss=keras.losses.MSE,metrics=['mse']) \n",
    "model.summary()\n",
    "history = model.fit(x_train_total, y_train_total,epochs=1000,batch_size=256,validation_data=(x_test_total,y_test_total),verbose=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'],label='train')\n",
    "plt.plot(history.history['val_loss'],label='test')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pr = model.predict(x_train_total)\n",
    "y_test_pr = model.predict(x_test_total)\n",
    "y_train_pr = scaler.inverse_transform(y_train_pr.reshape(-1,1))\n",
    "y_test_pr = scaler.inverse_transform(y_test_pr.reshape(-1,1))\n",
    "y_test =  scaler.inverse_transform(y_test_total.reshape(-1,1))\n",
    "y_train = scaler.inverse_transform(y_train_total.reshape(-1,1))\n",
    "print('train mse squared:',mean_squared_error(y_train_pr,y_train)) \n",
    "print('test mse squared:',mean_squared_error(y_test_pr,y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# veo como se ajustan a los train y test data para distintas localidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(test_data_scaled,df1,name):\n",
    "    n = len(test_data_scaled)\n",
    "    lag = tw\n",
    "    x_input=test_data_scaled[n-lag:].reshape(1,-1)\n",
    "    temp_input=list(x_input)\n",
    "    temp_input=temp_input[0].tolist()\n",
    "    lst_output=[]\n",
    "    n_steps=lag\n",
    "    i=0\n",
    "\n",
    "    while(i<10): \n",
    "        if(len(temp_input)>lag):\n",
    "            #print(temp_input)\n",
    "            x_input=np.array(temp_input[1:])\n",
    "            x_input=x_input.reshape(1,-1)\n",
    "            x_input = x_input.reshape((1, 1, lag))\n",
    "            #print(x_input)\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            print(\"{} day output {}\".format(i,yhat))\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            temp_input=temp_input[1:]\n",
    "            #print(temp_input)\n",
    "            lst_output.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "        else:\n",
    "            x_input = x_input.reshape((1, 1, lag))\n",
    "            yhat = model.predict(x_input, verbose=0)\n",
    "            temp_input.extend(yhat[0].tolist())\n",
    "            lst_output.extend(yhat.tolist())\n",
    "            i=i+1\n",
    "    \n",
    "    lst_output = scaler.inverse_transform(lst_output)\n",
    "    plt.plot(lst_output,'o',label='forecast')\n",
    "    plt.plot(df1,label='true data')\n",
    "    plt.title(dic_localidades[str(name)])\n",
    "    plt.xlabel('Días posteriores')\n",
    "    plt.ylabel('Casos/100mil hab')\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.savefig(dic_localidades[str(name)]+'_forecast_2.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = tf.keras.losses.MeanAbsolutePercentageError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    print(file)\n",
    "    data = pd.read_csv(mypath+str('/')+file,sep=\",\",quotechar='\"',na_values=[''])\n",
    "    data = data[\"incidenciaAcum14d\"]\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.to_numpy()\n",
    "    df_forecast = df[-20:-10]\n",
    "    df = df[:-20]\n",
    "    df_original = np.copy(df)\n",
    "    total_size = df.shape[0]\n",
    "    train_size = int(0.8*total_size)\n",
    "    test_size = total_size - train_size\n",
    "    train_data = df[:-test_size]\n",
    "    test_data = df[-test_size:]\n",
    "    train_data = scaler.transform(train_data.reshape(-1,1))\n",
    "    test_data = scaler.transform(test_data.reshape(-1,1))\n",
    "    #train\n",
    "    x_train, y_train = create_dataset(train_data,tw)\n",
    "    #test\n",
    "    x_test, y_test = create_dataset(test_data,tw)\n",
    "    x_train = x_train.reshape(x_train.shape[0],1,x_train.shape[1])\n",
    "    x_test= x_test.reshape(x_test.shape[0],1,x_test.shape[1])\n",
    "    y_train_pr = model.predict(x_train)\n",
    "    y_test_pr = model.predict(x_test)\n",
    "    y_train_pr = scaler.inverse_transform(y_train_pr.reshape(-1,1))\n",
    "    y_test_pr = scaler.inverse_transform(y_test_pr.reshape(-1,1))\n",
    "    y_train = scaler.inverse_transform(y_train.reshape(-1,1))\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1,1))\n",
    "    #imprimo mse para train y test\n",
    "    print('train rmse:',np.sqrt(mean_squared_error(y_train,y_train_pr))) \n",
    "    print('test rmse:',np.sqrt(mean_squared_error(y_test,y_test_pr)))\n",
    "    #imprimo r2\n",
    "    print('train mape:',r2_score(y_train,y_train_pr)) \n",
    "    print('test mape:',r2_score(y_test,y_test_pr)) \n",
    "    #imprimo mae\n",
    "    print('train mape:',mae(y_train,y_train_pr).numpy()) \n",
    "    print('test mape:',mae(y_test,y_test_pr).numpy()) \n",
    "    graph_predictions(tw,df_original,y_train_pr,y_test_pr,file)\n",
    "    forecast(test_data,df_forecast,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
